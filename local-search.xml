<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>REMIX —— Efficient Range Query for LSM-trees</title>
    <link href="/2021/12/15/REMIX:%20Efficient%20Range%20Query%20for%20LSM-trees/"/>
    <url>/2021/12/15/REMIX:%20Efficient%20Range%20Query%20for%20LSM-trees/</url>
    
    <content type="html"><![CDATA[<h2 id="0x00-Introduction"><a href="#0x00-Introduction" class="headerlink" title="0x00 Introduction"></a>0x00 Introduction</h2><p>LSM树在内存中缓存数据的更新，并周期性的将它们刷新到磁盘上，生成不可变的table文件，完成数据持久化。但是这也降低了查询效率，因为一个range中的key可能在不同的table文件中，并且由于较高的计算和I/O成本，这可能会潜在的降低查询速度。</p><p>学术界针对此问题已经做了大量工作来加快查询性能。为了加快点查询，每个table文件通常都与内存中的一个布隆过滤器相关联，以此来跳过不含目标key的table文件。但是布隆过滤器不能处理范围查询。</p><p>为了限制查询请求不得不访问的table数量，LSM树会在后台运行一个Compaction线程来不断对table进行归并排序。其中table的选择由Compaction策略来决定。</p><h4 id="Leveled-Compaction"><a href="#Leveled-Compaction" class="headerlink" title="Leveled Compaction"></a>Leveled Compaction</h4><p>Leveled Compaction策略被包括LevelDB和RocksDB在内的大量KV-Store使用。Leveled Compaction将多个较小的有序run归并排序为一个较大的有序run，以保证存在重叠的table文件数量小于指定的阈值。Leveled Compaction通过如上策略提供了较高的读效率，但因为其密集的归并排序策略，这也带来了比较高的「写放大」（Write Amplification，WA）。</p><h4 id="Tiered-Compaction"><a href="#Tiered-Compaction" class="headerlink" title="Tiered Compaction"></a>Tiered Compaction</h4><p>Tiered Compaction压缩策略等待多个大小相似的有序run，将它们合并为一个更大的run。其提供了更低的WA和更高的吞吐量，被Cassandra和ScyllaDB等KV-Store使用。但是Tiered Compaction不能有效降低存在key重叠的table数量，与Leveled Compaction相比这会带来更高的查询成本。</p><h4 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h4><p>其他的一些压缩策略虽然能比较好的平衡读写性能，但都没能同时实现高效的读写性能。</p><p>主要问题在于，为了限制有序run的数量，KV-Store必须进行归并排序和重写数据。<strong>现在的存储技术已经大大提高了随机读的性能。</strong>例如，对于Flash SSD来说，随机读的速度能达到顺序读速度的50%。例如3D-XPoint等新技术，为随机读和顺序读提供了几乎相同的性能。<strong>因此，KV-pairs不需要为了快速访问而进行物理排序；相反，KV-Store可以通过逻辑排序来提供高效的点查询和范围查询，以避免大量的重写操作。</strong></p><p>为此，本文设计了REMIX（Range-query-Efficient Multi-table Index）。传统的解决方案为了提高range query效率而在物理数据重写和昂贵的归并排序之间挣扎，而REMIX利用了节省空间的数据结构来记录跨多个table文件的全局排序视图。通过使用REMIXes，LSM-KVS可以在不牺牲查询性能的前提下，使用具有高写效率的压缩策略。</p><h2 id="0x10-Background"><a href="#0x10-Background" class="headerlink" title="0x10 Background"></a>0x10 Background</h2><p>LSM树是为了在持久化存储设备上实现高效的写速率而设计的。它将所有的更新缓存在内存中一个称为MemTable的数据结构中，以此来实现高效的写性能。</p><ul><li>minor compaction：当MemTable写满后，LSM树会通过minor compaction操作将缓存的所有key进行排序，并将其作为一个run刷新（flush）到持久化的存储设备上。此操作不需要对存储设备上的现有数据进行合并，只需要批量顺序写入，因此实现了高效写性能；</li><li>major compaction：有序的run之间可能会存在重叠的key范围，这时一个点查询（point query）就不得不去检查所有可能的run，这带来了较大的查询开销。为了限制存在重叠的run的数量，LSM树通过major compaction操作将几个存在重叠的run归并排序为一个run。</li></ul><p>压缩策略（compaction strategy）决定了如何选择table进行major compaction。最流行的两种压缩策略是leveled compaction和tiered compaction：</p><p><strong>leveled compaction</strong> ：</p><p>使用此方法的数据库具有多层的结构，每层都维持一个由一个或多个table组成的有序run。其中某一个Level $L_n$的容量是其前一个Level $L_{n-1}$的几倍（通常是10），这使得一个庞大的KV-store被组织为仅有几个Level组成（通常是5～7个）。Leveled Compaction带来了相对高效的读性能，但也会造成写性能较差。Leveled Compaction从相邻的Level（例如$L_n$和$L_{n+1}$）中选择重叠的table进行合并，并在较大的Level（例如$L_{n+1}$）中生成新的table。因为不同Level之间的容量呈指数级增长，因此一个table的键范围通常与下一个Level的多个table重叠。因此大多数写操作是在$L_{n+1}$中重写已有的数据，这在实践中导致了高达40倍的高「写放大比」（write amplification ratio， WA ratio，即实际写入磁盘的数据量与用户请求写入的数据量的比值）。</p><p>图1展示了一个Leveled Compaction的例子，每个table包含1～2个key。如果$L_1$中的第一个table被选择与$L_2$中的前两个table进行归并排序，那么$L_2$中的5个key将被重写（也即<code>table(4,21,38)</code>要与<code>table(6,26)</code>和<code>table(31,40,46)</code>进行归并排序，此时$L_2$两个table中的5个key都需要重写）。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_F_1.png" alt="image-20211213113120768"></p><p><strong>tired compaction</strong>:</p><p>如图2所示，在使用tired compaction的LSM树中，<strong>在一个level中可以缓存多个存在重叠的有序run</strong>。通常，一个level中run的数量会通过一个阈值$T$来限制（$T&gt;1$）。如果在一个level（例如$L_n$）中的有序run的数量超过阈值$T$，那么**$L_n$中的所有run会被归并排序为$L_{n+1}$中的一个新的run，$L_{n+1}$中的数据不会被重写**。因此，在使用tired compaction的LSM树中，写放大比为$O(L)$，其中$L$是level的数量。但是，由于在每个level中可能会有多个重叠的有序run，一个点查询（point query）可能会需要去检查$T\times L$个table，这也导致了更低的查询性能。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_F_2.png" alt="image-20211213143711989"></p><p>LevelDB和RocksDB中的范围查询是通过使用一个迭代器结构在多个table中导航实现的，就好像所有的key都在一个有序run中。一个范围查询首先会使用带有搜索键（seek key，也即目标key范围的下边界）的搜索操作（seek operation）来初始化迭代器。「查找操作」首先定位迭代器，使其指向存储中大于等于seek key的最小key，这表示范围查询的目标key。next操作会往前推进迭代器，使其指向有序run中的下一个key。因此，可以使用一系列next操作来检索目标范围内的key，直到满足某个条件（例如达到查询的数量或者到达查询范围的末尾）。由于有序run是按时间顺序生成的，因此一个目标key可能会在任何一个run中，相应的，迭代器必须追踪所有的有序run。</p><p>图1展示了一个使用leveled compaction的LSM树的查找示例。为了查找key 67，需要在每个run中使用二分查找来识别满足$key \geq seek,key$的最小key。每个识别的key都用一个光标（cursor）来标记。然后使用最小堆结果来对这些键进行归并排序，从而选择$L_2$中的key 67。随后，每个next操作都会比较光标标记下的key，返回其中最小的一个key，然后向前移动其对应的光标。如图1右上角所示，此过程会呈现key的全局排序视图。在图1的例子中，所有的3个level都会被访问以进行归并排序。图2使用的是tired compaction，但查询过程类似，不过在这个例子中有6个run需要被访问，其查找代价相对更大。</p><h2 id="0x20-REMIX"><a href="#0x20-REMIX" class="headerlink" title="0x20 REMIX"></a>0x20 REMIX</h2><p>对多个有序run的范围查询会动态构建底层table的排序视图，以便可以按照排序顺序来检索key。事实上，排序视图继承了table文件的不变性，并会在任何table被删除或替代前保持有效。然而现有的基于LSM的KV-Store并没有利用这种不变性的特点，排序视图会在查询时反复重建和丢弃，这会带来密集的计算和I/O操作并导致较差的查找性能。</p><p>REMIX的动机是利用table文件的不变性，保留底层table文件的排序视图，以在将来的查询操作中重用它们。</p><p>如果记录每个key及其位置来保留排序视图，则存储的元数据可能会显著膨胀，从而导致读写性能受损。因此，为了避免这个问题，REMIX的数据结构必须是空间高效的。</p><h3 id="0x21-The-REMIX-Data-Structure"><a href="#0x21-The-REMIX-Data-Structure" class="headerlink" title="0x21 The REMIX Data Structure"></a>0x21 The REMIX Data Structure</h3><p>图3上部展示了一个由三个run组成的排序视图的示例，排序视图用向量将15个key连接起来。</p><p>为了构建一个REMIX，首先将排序视图的key划分到多个segment中，每个segment都包含固定数量的key。</p><p>每个segment都与一个<code>anchor key</code>，一组<code>cursor offset</code>以及一组<code>run selectors</code>关联。其中：</p><ul><li>anchor key：表示segment中的最小key；</li><li>cursors offsets：所有cursor共同组成了排序视图上的稀疏索引，每个cursor offset都与一个run对应，并记录了此run中大于等于anchor key的最小key；</li><li>run selectors：segment中的每个key都有一个对应的run selector，run selector用来指示对应的key位于哪个run上；</li></ul><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_F_3.png" alt="image-20211213163458320"></p><p>REMIX的迭代器不使用最小堆。在REMIX中，一个迭代器包含「一组cursor」和「一个current pointer」。</p><ul><li>cursors：每个cursor都与一个run相关联，并指向该run中一个key的位置。</li><li>current pointer：选择并指向一个run selector，该run的cursor决定了此时到达的key；</li></ul><p>在REMIX中，使用一个迭代器来查找一个key需要三步：</p><ol><li>找到target segment。在anchor keys上执行二分查找，以确定包含seek key的target segment，也即找到满足anchor_key$\leq$seek_key的segment；</li><li>初始化迭代器。将迭代器初始化为指向anchor key。具体来说就是，用target segment的cursor offsets来初始化迭代器的cursors，并将迭代器的current pointer指向target segment的第一个run selector；</li><li>可以通过线性扫描有序视图来查找 target key。为了移动迭代器，将当前key的cursor前进以跳过当前key，同时，current pointer也需要前进以指向下一个run selector；</li></ol><p>来看一个例子。如图3所示，底部的四个方框代表编码了有序视图的REMIX元数据（需要注意的是括号中的key并不是元数据的一部分）。查找key 17的操作如下：</p><ol><li>查找target segment。通过二分查找，第二个segment被选中，其包含的key为<code>(11,17,23,29)</code>。</li><li>初始化迭代器。根据target segment的cursors offset，也即<code>(1,2,1)</code>，将迭代器的cursors放在$R_0$、$R_1$和$R_2$的键11、17和31上；与此同时，将迭代器的current pointer设置为指向target segment的第一个run selector（图中的第5个run selector，也就是第二个segment的第一个run selector，为0，指向的是第一个run），这意味着current key也即key 11在$R_0$的cursor所在的位置。</li><li>current key 11小于17，迭代器需要向前移动，以查找下一个满足$k\geq17$的最小key。为了向前移动迭代器，$R_0$的cursor向前移动跳过key 11，停在key 23上，此时迭代器的cursors offset为<code>(2,2,2)</code>；然后current pointer向前移动到target segment的第二个run selector（图中的第6个run selector，也就是第二个segment的第二个run selector，为1，指向的是第二个run）。重复此操作最终找到目标key。</li></ol><h3 id="0x22-Efficient-Search-in-a-Segment"><a href="#0x22-Efficient-Search-in-a-Segment" class="headerlink" title="0x22 Efficient Search in a Segment"></a>0x22 Efficient Search in a Segment</h3><p>seek操作通过在anchor keys上执行二分查找来定位target segment以初始化迭代器，然后再在有序视图上向前扫描来查找target key。显然，增加segment的大小能减少anchor key的数量，并进一步加速二分查找。但这也会降低查找target key的速度，因为需要在一个更大的target segment上访问更多的key来查找target key。</p><p>为了解决潜在的性能问题，REMIX在target segment中也使用二分查找来最小化查找开销。</p><p>目前为止，使用二分查找的地方有：</p><ul><li>anchor keys之间：在anchor keys上执行二分查找来定位target segment；</li><li>target segment内部：在target segment上执行二分查找来定位target key；</li></ul><p>但是，<strong>要在segment上使用二分查找则必须能随机访问segment中的每个key</strong>。</p><p>由上文可知，segment中的每个key属于一个run，并用run selector指示其所在的run。为了访问一个key，我们需要将run的cursor放在正确的位置上，可以通过计算在「该key之前」并「与其相同的run selector的出现次数」，然后将cursor移动相应的次数来将cursor放置到正确的位置上。通过这种类似数组的访问模式，我们可以实现segment中每个key的随机访问。</p><p>图4展示了一个有16个key的segment，所有的key位于不同的4个run上。Run Selectors下面的Occurrences表示该segment中某个key之前还有多少个key与它位于同一个run中。例如，key 41是该segment中位于$R_3$上的第3个key，也即在key 41之前还有2个key位于$R_3$上（key 5和key 23），所以其Occurrences值为2。要查找访问 key 41，我们只需要初始化$R_3$的cursor offset，然后向前移动两次跳过key 5和key 23即可，这便实现了key 41的随机访问。</p><p>简单记录一个segment中二分查找的过程，还是例如要查找key 41，过程如下：</p><ol><li>第一次执行二分查找。需要检索的是该segment中的第8个key（总共16个key，所以第一次检索第8个key），该key对应的run selector为3，occurrence值为3，然后初始化$R_3$的cursor offset，然后向前移动3次即可访问该key，该key为43。该key不是我们要找的key 41，且大于key 41；</li><li>第二次执行二分查找。由第一次可知，需要在segment中的第1～7个key进行二分查找，因此第二次二分查找需要检索第4个key，该key对应的run selector为2，occurrence为0，初始化$R_2$的cursor offset，向前移动0次即可访问该key，该key为31，仍不是要查找的key。</li><li>第三次执行二分查找。方法同上，本次检索第6个key，仍不是要查找的key。</li><li>第四次执行二分查找。方法同上，本次检索第7个key，是本次查找的target key，查找结束。</li></ol><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_F_4.png" alt="image-20211214102822250"></p><p>在segment中使用二分查找可以减少查找过程中key比较的次数，但是，查找路径中的key可能位于不同的run上，而若相应的block没有被缓存的话则必须通过独立的I/O请求来读取这些key。例如，我们上面的查找例子中，只需要进行4次二分查找也即需要比较4次，但是需要访问3个run（4次比较中的key位于3个不同的run上）。事实上，很有可能该segment中位于同一个run上的几个key可能在同一个block中，因此，在一次比较完成后，移动到下一个run之前，可以通过比较该run上位于同一个block中的key来进一步缩小查找区间。例如，假设$R_3$中的key 41和key 43位于同一个block中，且我们要查找的target key也是key 43，则第一次需要检索key 41进行比较，比较完成后，在移动到$R_0$上访问Key 71比较之前，可以顺便比较一下与它位于同一block中的key 43，而这个key正好是我们需要的target key，此次访问直接将查找区间缩小到了target key。</p><h3 id="0x23-Search-Efficiency"><a href="#0x23-Search-Efficiency" class="headerlink" title="0x23 Search Efficiency"></a>0x23 Search Efficiency</h3><p>REMIX在以下三个方面改进了范围查询：</p><ul><li>REMIX使用二分查找来定位target key。REMIX提供了一个由多个run组成的全局视图，只需要一个二分查找就可以在该全局视图中定位到target key。</li><li>REMIX移动迭代器时不需要key的比较。REMIX通过使用预先记录的run selector来更新迭代器的cursors和current pointer，可以直接切换到下一个KV对。此过程不需要任何的key比较。相比之下，传统LSM-KVS的迭代器会维护一个最小堆，以对来自多个重叠run的key进行归并排序，在这种情况下，一个next操作需要从多个run中读取key进行比较。</li><li>REMIX会跳过不在查找路径中的run。seek操作会在target segment中执行二分查找，而且在查询时仅会访问那些包含查找路径上的key的run。</li></ul><p>此外，显著降低的查询成本允许在不使用布隆过滤器的情况下对由REMIX索引的多个有序run进行高效的点查询。</p><h3 id="0x24-REMIX-Storage-Cost"><a href="#0x24-REMIX-Storage-Cost" class="headerlink" title="0x24 REMIX Storage Cost"></a>0x24 REMIX Storage Cost</h3><p>REMIX的元数据由三个组件组成：anchor keys、cursor offsets、run selectors。为了对其存储成本进行评估，现给出如下定义：</p><ul><li>$D$表示一个segment中最多能包含的key的数量，因为每个key都有一个对应的run selector，那么run selector的数量相应的也为D；</li><li>$S$表示cursor offset的大小，单位为字节；</li><li>$H$表示被REMIX索引的run的数量；</li><li>$\bar{L}$表示anchor key的平均大小，单位为字节；</li></ul><p>由上面的定义可得：</p><ul><li>$S \times H$ 表示一个segment中所有的cursor offset所占用的空间大小，单位为字节；</li><li>$\left \lceil log_2(H) \right \rceil/8$ 表示一个run selector所占用的空间大小，单位为字节；</li><li>$D \times \left \lceil log_2(H) \right \rceil/8$ 表示一个segment中所有run selector所占用的空间大小，单位为字节；</li></ul><p>那么由上面的定义和推理可得，REMIX中各个组件占用的空间大小为（单位为字节）：</p><ul><li>anchor keys ：$\bar{L}$个字节；</li><li>cursor offsets：$SH$个字节；</li><li>run selectors：$D \times \left \lceil log_2(H) \right \rceil/8$个字节</li></ul><p>将REMIX所占用的总空间大小摊分到每个key上（也即将占用总空间大小除D），则每个key所额外占用的空间大小为：<br>$$<br>(\bar{L}+SH)/D + \left \lceil log_2(H) \right \rceil/8<br>$$<br>表1展示了在不同工作负载（$D$，也即segment中的key数量不同）下的REMIX空间开销。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_T_1.png" alt="image-20211214203246297"></p><h2 id="0x30-RemixDB"><a href="#0x30-RemixDB" class="headerlink" title="0x30 RemixDB"></a>0x30 RemixDB</h2><p>为了评估REMIX的性能，本文设计实现了一个名为RemixDB的LSM-KVS。RemixDB为了实现最好的的读性能而采用了tiered compaction。现实世界中的工作负载通常表现出高空间局部性。最近的研究表明，分区存储布局可以有效降低实际工作负载下的压缩成本。RemixDB采用了这种方法，将key空间划分到多个不重叠的分区中。每个分区中的table文件由一个REMIX进行索引，以提供分区的排序视图。通过这种方式，RemixDB本质上是一个使用tiered compaction的单层LSM树。RemixDB不仅继承了tiered compaction的写效率，而且在REMIXes的帮助下实现了高效读性能。</p><p>图5显示了RemixDB的系统组件。分区中的压缩会创建一个新版本的分区，其中包含新旧table文件和新的REMIX文件，旧版本在压缩后会被垃圾回收。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_F_5.png" alt="image-20211214210216968"></p><p>在多层LSM树的设计中，一个MemTable的大小通常只有几十MB，接近SSTable的默认大小。而在分区存储布局中，更大的MemTable可以在触发压缩前吸收更多的更新，这能帮助降低WA（写放大，Write Amplification），且MemTable和WAL的空间成本几乎不变，考虑到现在数据中心的大内存和存储，这是适中的。在RemixDB中，MemTable的最大大小被设置为4GB。</p><h3 id="0x31-The-Structures-of-RemixDB-Files"><a href="#0x31-The-Structures-of-RemixDB-Files" class="headerlink" title="0x31 The Structures of RemixDB Files"></a>0x31 The Structures of RemixDB Files</h3><h4 id="Table-Files"><a href="#Table-Files" class="headerlink" title="Table Files"></a>Table Files</h4><p>图6显示了RemixDB中的table文件格式。</p><ul><li>data block的默认大小为4Kb；</li><li>无法存放在一个data block中的大KV对会存放在整数倍的data block中；</li><li>每个data block在其首部都有一个记录KV数据偏移量的数组，以随机访问它所存储的每个KV数据；</li><li>metadata block是由多个8-bit值所组成的数组，每个值记录了一个4KB的block中所存储的key的数量，因此一个block最多可以存储255个KV对；</li></ul><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_F_6.png" alt="image-20211214212026005"></p><h4 id="REMIX-Files"><a href="#REMIX-Files" class="headerlink" title="REMIX Files"></a>REMIX Files</h4><p>图7显示了RemixDB中的REMIX文件格式。</p><p>REMIX中的anchor keys被组织在一个类似B+树的不可变索引中，这有助于对anchor key进行二分查找。每个anchor key都与一个segment ID相关联，segment ID标识了该segment的cursor offsets和run selectors。cursor offset由16位的block index和8位的key index索引组成，如图7的blk-id和key-id所示。因为每个cursor offset都与一个run对应，所以图7中的每个segment的查找路径中都只有两个run。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_F_7.png" alt="image-20211214213100203"></p><p>一个key的多个版本可能存在于同一个分区的不同table文件中，范围查询必须跳过旧版本返回每个key的最新版本。因此，在REMIX中，一个key的多个版本在排序视图上从最新到最旧排序，并且每个run selector的最高位被保留以用来区分旧版本和新版本。在前向扫描过程中总是先遇到最新版本，通过检查run selector的最高位可以在不进行比较的情况下跳过旧版本。</p><h3 id="0x32-Compaction"><a href="#0x32-Compaction" class="headerlink" title="0x32 Compaction"></a>0x32 Compaction</h3><p>在每个分区中，压缩进程根据进入分区的「新数据的大小」和「现有table文件的布局」来估计压缩成本，然后根据该成本执行以下不同的操作：</p><ul><li>Abort：取消分区的压缩，并将新数据保留在MemTable和WAL中；</li><li>Minor Compaction：将新数据写入到一个或多个新的table中，而不重写现有的table文件；</li><li>Major Compaction：将新数据与部分或全部现有table文件合并；</li><li>Split Compaction：将新数据与所有现有数据合并，并将分区拆分为几个新分区；</li></ul><h4 id="Abort"><a href="#Abort" class="headerlink" title="Abort"></a>Abort</h4><p>compaction后，任何看到新table文件的分区都需要重建其REMIX。Minor Compaction后会在一个分区中创建一个小的table文件，这会导致REMIX的重建并带来高I/O开销。</p><p>为了减少I/O开销，如果预估的I/O开销大于阈值，RemixDB可以终止分区的压缩。在这种情况下新的KV数据会被继续保留在MemTable和WAL中，等待下一次压缩。</p><p>但是在极端的情况下，当RemixDB终止了大多数分区的压缩时，压缩过程将无法有效的将数据移动到分区中。为了避免这个问题，RemixDB进一步限制可以留在MemTable和WAL中的新数据的大小，如果超过MemTable大小的15%。因此，在达到此限制之前，RemixDB终止具有高I/O开销的压缩过程，达到上限之后不再继续终止，允许压缩。</p><h4 id="Minor-Compaction"><a href="#Minor-Compaction" class="headerlink" title="Minor Compaction"></a>Minor Compaction</h4><p>Minor Compaction将Immutable MemTable中的新KV数据写到分区的新table文件中，然后重建REMIX，这个过程不需要重写分区中现有的table文件。当压缩后的Table文件的数量（现有table加上新建table的数量）低于阈值T时，使用Minor Compaction，在RemixDB的实现中T为10。图8显示了一个minor compaction的例子。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_F_8.png" alt="image-20211215103811139"></p><h4 id="Major-Compaction"><a href="#Major-Compaction" class="headerlink" title="Major Compaction"></a>Major Compaction</h4><p>当分区中的table文件的预期数量超过阈值T时，进行Major Compaction，将现有的table文件归并排序为较少的table文件。随着table文件数量的减少，后面就可以执行minor compaction。Major Compaction的效率可以通过输入table文件的数量和输出table文件的数量之比来估计。</p><p>图9显示了一个Major Compaction的示例，新数据与现有的三个较小的table文件进行合并，合并后只创建了一个新的table文件，此时$ratio=3/1$。Major Compaction会选择可以产生最高输入输出比的方案进行压缩。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_F_9.png" alt="image-20211215105042707"></p><h4 id="Split-Compaction"><a href="#Split-Compaction" class="headerlink" title="Split Compaction"></a>Split Compaction</h4><p>当分区中充满了large table时，Major Compaction可能无法有效减少分区中的table数量，这可以通过较低的输入输出比来预测。在这种情况下，应该将分区拆分为多个分区，这样每个分区中的table文件数量都能大大减少。Split Compaction会将新数据与分区中的所有现有table文件进行合并，并生成新的table文件以形成多个新分区。图10显示了一个split compaction的例子。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/REMIX_F_10.png" alt="image-20211215105601167"></p><h3 id="0x33-Rebuilding-REMIXes"><a href="#0x33-Rebuilding-REMIXes" class="headerlink" title="0x33 Rebuilding REMIXes"></a>0x33 Rebuilding REMIXes</h3><p>在具有高空间局部性的现实世界工作负载下，分区存储布局可以有效的最小化压缩成本。具体来说，RemixDB可以在少数分区中吸收大部分更新，并且可以避免接受具有较少更新的分区中的压缩。但是，如果工作负载缺少空间局部性，则不可避免的会有大量分区在较少更新的情况下执行压缩。Tiered Compaction可以最大限度的减少这些分区中的写入，但是在分区中重建REMIX仍然需要读取现有的table。</p><p>在分区中重建REMIX时，现有table已经被现有的REMIX索引，并且可以将这些table视为一个有序run，因此，重建过程相当于对两个已排序的run进行归并排序，其中一个来自现有数据，另一个来自新数据。</p>]]></content>
    
    
    <categories>
      
      <category>Key-Value Store</category>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>Key-Value Store</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C++字符串输入的使用及混合数字读取可能出现的问题</title>
    <link href="/2021/12/10/C++%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BE%93%E5%85%A5/"/>
    <url>/2021/12/10/C++%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BE%93%E5%85%A5/</url>
    
    <content type="html"><![CDATA[<h2 id="0x00-字符串输入"><a href="#0x00-字符串输入" class="headerlink" title="0x00 字符串输入"></a>0x00 字符串输入</h2><p><code>cin</code>使用空白来确定字符串的结束位置，这意味着<code>cin</code>在获取字符数组输入时只读取一个单词。读取该单词后，<code>cin</code>将该字符串放到数组中，并自动在结尾添加空字符<code>\0</code>。</p><p>注：C风格字符串以空字符结尾，空字符被写作<code>\0</code>，其ASCII码为0，用来标记字符串的结尾。</p><h2 id="0x10-每次读取一行字符串输入"><a href="#0x10-每次读取一行字符串输入" class="headerlink" title="0x10 每次读取一行字符串输入"></a>0x10 每次读取一行字符串输入</h2><p>大多时候，每次读取一个单词不是最好的选择。</p><p>头文件<code>istream</code>中的类（如<code>cin</code>）提供了一些面向「行」的类成员函数：<code>getline()</code>和<code>get()</code>。这两个函数都读取一行输入，直至到达换行符。然而，随后<code>getline()</code>将<strong>丢弃换行符</strong>，而<code>get()</code>将换行符<strong>保留在输入序列</strong>中。</p><h3 id="0x11-面向行的输入：getline"><a href="#0x11-面向行的输入：getline" class="headerlink" title="0x11 面向行的输入：getline()"></a>0x11 面向行的输入：<code>getline()</code></h3><p><code>getline()</code>函数读取整行，它使用通过<strong>回车键输入的换行符</strong>来确定输入结尾。要调用这个函数，可以使用<code>cin.getline()</code>。</p><p>该函数有两个参数：</p><ul><li>第一个参数是用来存储输入行的数组的名称；</li><li>第二个参数是要读取的字符数；</li></ul><p><code>getline()</code>成员函数在读取指定数目的字符或遇到换行符时停止读取。</p><p>例如，在使用该函数将姓名读取到一个包含20个元素的<code>name</code>数组时，可以这样使用: <code>cin.getline(name, 20);</code></p><h3 id="0x12-面向行的输入：get"><a href="#0x12-面向行的输入：get" class="headerlink" title="0x12 面向行的输入：get()"></a>0x12 面向行的输入：<code>get()</code></h3><p>cin类还有一个名为get()的成员函数，该函数有几种变体。</p><p>其中有一种变体的工作方式与getline()类似，它们接受的参数相同，解释参数的方式也一样，并且都读到行尾。但get并不再读取并丢弃换行符，而是将其留在输入队列中。假设我们连续两次使用get()：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs C++">cin.<span class="hljs-built_in">get</span>(name1, len);<br>cin.<span class="hljs-built_in">get</span>(name2, len); <span class="hljs-comment">// 出现问题，上一个get将换行符留在了输入队列，导致此get看到的第一个字符就是换行符，不能继续读取想要读取的内容</span><br></code></pre></td></tr></table></figure><p>由于第一次调用后，将换行符留在了输入队列中，因此第二次调用时看到的第一个字符便是换行符，此时，因为<code>get(name2，len)</code>并不能读取换行符，所以它会认为已经到达行尾了，并没有发现任何可以读取的内容。因此，如果不借助其他帮助，第二次调用将无法跨过换行符来读取内容。</p><p>而get()函数的另一种变体，使用不带任何参数的cin.get()调用可以读取下一字符（可以是换行符）。因此可以使用它来处理换行符。例如：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs C++">cin.<span class="hljs-built_in">get</span>(name1, len);<br>cin.<span class="hljs-built_in">get</span>();  <span class="hljs-comment">// 读取换行符</span><br>cin.<span class="hljs-built_in">get</span>(name2, len); <br></code></pre></td></tr></table></figure><p>另一种方式是将两个类成员函数变体拼接起来，如下所示：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs C++">cin.<span class="hljs-built_in">get</span>(name1, len).cin.<span class="hljs-built_in">get</span>(); <span class="hljs-comment">// 读取字符串后继续读取掉换行符</span><br>cin.<span class="hljs-built_in">get</span>(name2, len); <br></code></pre></td></tr></table></figure><p>之所以可以这样使用是因为<code>cin.get(name1, len)</code>返回一个<code>cin</code>对象，该对象可以用来继续调用<code>get()</code>函数。</p><h2 id="0x20-混合输入字符串和数字所带来的问题"><a href="#0x20-混合输入字符串和数字所带来的问题" class="headerlink" title="0x20 混合输入字符串和数字所带来的问题"></a>0x20 混合输入字符串和数字所带来的问题</h2><p>C++中混合输入<strong>面向行的字符串</strong>和<strong>数字</strong>会导致问题。例如：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">int</span> age;<br><span class="hljs-keyword">char</span> name[<span class="hljs-number">30</span>];<br>cin&gt;&gt;age;<br>cin.<span class="hljs-built_in">getline</span>(name, <span class="hljs-number">30</span>);<br></code></pre></td></tr></table></figure><p>这样使用会导致<code>cin.getline()</code>读取到一个空字符串，原因在于当<code>cin</code>在读取<code>age</code>时会将回车键生成的换行符留在了输入队列，而后面的<code>cin.getline()</code>在看到换行符后，将认为这是一个空行，并将一个空字符串数组赋给<code>name</code>数组。</p><p>解决方法是：在使用<code>cin.getline(name, 30)</code>读取整行字符串之前，先使用<code>cin.get()</code>读取并丢弃换行符。</p>]]></content>
    
    
    <categories>
      
      <category>C/C++</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C/C++</tag>
      
      <tag>编程语言</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AC-Key——Adaptive Caching for LSM-based Key-Value Stores</title>
    <link href="/2021/12/10/AC-Key%E2%80%94%E2%80%94Adaptive%20Caching%20for%20LSM-based%20Key-Value%20Stores/"/>
    <url>/2021/12/10/AC-Key%E2%80%94%E2%80%94Adaptive%20Caching%20for%20LSM-based%20Key-Value%20Stores/</url>
    
    <content type="html"><![CDATA[<h2 id="0x00-Introduction"><a href="#0x00-Introduction" class="headerlink" title="0x00 Introduction"></a>0x00 Introduction</h2><p>大多数现有的KVS都采用了Log-Structured Merge(LSM) tree来提高写操作的性能，然而LSM的结构特性使得读操作的性能受到了很大的影响。当在多个Level中查找一个key时会引起多次磁盘的I/O操作，带来较大的性能开销。</p><p>而大部分工作负载都有访问局部性的特征，缓存便是利用这一特性来提高读操作性能的主要技术。针对企业级工作负载的研究发现，无论是「point lookup」还是「range query」都展现出了「hot spots」的现象。</p><p>因此，为基于LSM的KV-Store设计高性能的缓存方案主要有两个技术挑战：</p><ul><li>首先，LSM分层的设计使得缓存不同Level的KV数据所带来的收益不同，也即能够节省的磁盘I/O次数不同；</li><li>其次，两种读操作「point lookup」和「range query」对缓存的需求不一致。「Point lookup」更偏向于缓存一个「key-value pair」，这种方式相对节省空间；当value过大时，一种折中的方案是选择缓存「key- pointer pair」，也即缓存「key」和「value的磁盘指针」，指针相对value所占用的空间更小。而缓存单个零星的「key-value pair」无法为「range query」服务，因此只能通过缓存「block」来支持「range query」；</li></ul><p>因此，很难在缓存KV、KP和Block之间进行权衡，它们都有其各自适合的工作负载。此外，设计一种可以根据工作负载进行自适应的缓存方案是困难的。</p><p>现有的缓存方案通常只选择缓存KV、KP和Block中的一种或两种，而且为每种缓存类型都分配了固定大小的缓存空间。因此，这些缓存方案无法适应多样的工作负载，也不能根据工作负载的变化来调整缓存空间的大小。此外，目前还没有针对LSM中特有的异构缓存成本的解决方案。</p><p>本文全面分析研究了对缓存KV、KP和Block之间的权衡，并提出了「 AC-Key, Adaptive Caching for LSM-based Key-Value Stores」来结合它们在面临各种工作负载时的优势。AC-Key为每种类型的条目（KV、KP和Block）都使用专门设计的单独缓存组件。每个缓存组件的大小可以根据文章提出的「 <em>hierarchical adaptive caching</em> algorithm，分层自适应缓存算法」进行调整（使用「ghost cache」来指导其大小调整）。</p><p>此外，AC- Key利用一种新颖的缓存效率因子来评估不同的<strong>缓存成本</strong>和<strong>收益</strong>，以指导<strong>不同缓存组件之间的边界调整</strong>以及<strong>缓存组件内部的替换策略</strong>。</p><h2 id="0x10-Background-amp-Related-work"><a href="#0x10-Background-amp-Related-work" class="headerlink" title="0x10 Background &amp; Related work"></a>0x10 Background &amp; Related work</h2><h3 id="0x11-LSM-Tree-Based-Key-Value-Store"><a href="#0x11-LSM-Tree-Based-Key-Value-Store" class="headerlink" title="0x11 LSM-Tree-Based Key-Value Store"></a>0x11 LSM-Tree-Based Key-Value Store</h3><p>基于LSM tree的Key-Value store（LSM- KVS）的流行实现，例如LevelDB和RocksDB，都由两部分组成：</p><ul><li>Memory Component，或者说MemTable，通常使用原地排序（in-place sorted）的数据结构来实现，例如「skip-list」或「$B^+$ tree」；</li><li>Storage Component，通常被实现为存储有序run（压缩的Key-Value对组成）的多层文件；</li></ul><p>如图1所示，每个Level被划分为多个「sorted string table files」也即SSTs，每个SST都有配置的大小限制，通常为2MB～64MB。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/AC-Key_F_1.png" alt="image-20211210110007408"></p><p>除了$L_0$之外，每个Level都是一个单独的有序run，其中，SST之间具有不相交的key范围。在有序run中，一个SST的所有key-value对被划分到多个数据块（data block）中。SST中每两个相邻数据块之间的边界key存储在索引数据块（index block）中，索引数据块在SST中有相应的数据块偏移量（data block offset）。此外，每个SST还包含一个「bloom filter block」（BF block，布隆过滤块）来确定SST中是否存在某个key，以避免不必要的磁盘I/O操作。「Block」是LSM- KVS中的基本磁盘I/O单元。</p><p>在LSM-KVS中有两种类型的读操作，分别是「point-lookup」（也即<code>Get</code>）和「range query」（也即<code>Scan</code>）。<code>Get</code>按照下面的顺序来查找指定key的value：先是检查内存中的MemTable，然后是磁盘上$L_0$中从新到旧的每个SST，$L_0$中查找失败则继续查找$L_1$~$L_N$。</p><p>如果在内存MemTable中就找到了要找的key，则不需要任何的磁盘访问就可以返回相应的value。否则，则需要在磁盘组件中查询SST文件。查询SST时会先检查对应的布隆过滤器，如果布隆过滤器指示该SST不存在指定key的话则跳过此SST。反之，则读取索引块来定位key所在的具体数据块，最后读取相应数据块来查找此key。因此，在一个SST中查找一个key最多需要三次磁盘I/O操作：</p><ul><li>第一次读Bloom filter block；</li><li>第二次读Index block；</li><li>第三次读data block；</li></ul><h3 id="0x12-Related-Work"><a href="#0x12-Related-Work" class="headerlink" title="0x12 Related Work"></a>0x12 Related Work</h3><h4 id="a-Caching-Schemes-in-LSM-KVS-LSM-KVS中的缓存方案"><a href="#a-Caching-Schemes-in-LSM-KVS-LSM-KVS中的缓存方案" class="headerlink" title="a. Caching Schemes in LSM-KVS (LSM-KVS中的缓存方案)"></a>a. Caching Schemes in LSM-KVS (LSM-KVS中的缓存方案)</h4><p>如前文所述，在LSM-KVS中有三种类型的条目可以被缓存：KV，KP和Block（如图2所示）。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/AC.png" alt="image-20211209162834502"></p><p>LevelDB仅采用了「Block Cache」（如图2a所示），这里的「Block」可以是：data block，index block，以及Bloom Filter block。「Block Cache」中的「block」使用<code>SST file ID</code>和<code>block offset</code>（例如，<code>&lt;SstID | BlockOffset&gt;</code>）来索引。「Block Cache」可以同时服务「point lookup」和「range query」。虽然Block Cache也能够服务point lookup，但是空间成本太高，因为只有「Block」中的小部分「key-value」才会被频繁访问。</p><p>RocksDB同时支持<code>Block Cache</code>和<code>KV Cache</code>（如图2b所示）。<code>KV Cache</code>缓存的KV对可以服务于<code>point lookup</code>。但是RocksDB中<code>Block Cache</code>和<code>KV Cache</code>的缓存大小都是预定义好的，一旦配置完成就无法动态调整。</p><p>Cassandra支持<code>KV Cache</code>和<code>KP Cache</code>（如图2c所示），但是不支持<code>Block Cache</code>。在KP Cache中，<code>value</code>在磁盘中的位置作为指针被缓存到内存中。一旦命中了<code>KP Cache</code>，因为<code>KP Cache</code>中已经保存了<code>value</code>在磁盘中的位置，所以仅需要一次额外的磁盘I/O就可以完成「point lookup」。与<code>KV Cache</code>相比，在面临较大的<code>value</code>时，<code>KP Cache</code>以一次额外的磁盘I/O为代价带来了更低的内存空间成本（也即空间效率更高）。但是，与<code>KV Cache</code>相似，<code>KP Cache</code>也不支持「range query」。</p><h4 id="b-General-Caching-Algorithms-通用的缓存算法"><a href="#b-General-Caching-Algorithms-通用的缓存算法" class="headerlink" title="b. General Caching Algorithms (通用的缓存算法)"></a>b. General Caching Algorithms (通用的缓存算法)</h4><p>Adaptive Replacement Cache (ARC，自适应替换缓存)是为管理DRAM中的「页缓存」而设计的「动态页面替换算法」。如图3所示，ARC将缓存空间划分为两个部分：「recency cache」 和 「frequency cache」，它们每个都是一个LRU（Least Recently Used，最近最少使用）缓存。 在ARC中，当一个页面第一次被访问时，将会被放到「recency cache」中，如果此页面在被移出「recency cache」前（也即被替换前）得到了第二次访问，则此页面会被认为是一个「frequently accessed page」，然后会将其进一步迁移到「frequency cache」。</p><p>recency cache和frequency cache之间的空间分配是动态的。ARC使用两个ghost cache来分别存储从recency cache和frequency cache 中替换出来的页面的元数据。ghost cache中缓存的页面元数据将会作为将来调整各部分缓存空间的参考。与real cache（也即存储完整页面的cache）相比，ghost cache的空间大小几乎可以忽略不计，因为它们只存储页面编号。</p><p>recency ghost cache的命中意味着recency cache的缓存空间应该更大，如图3上部所示，目标边界应该向左移动，反之亦然。因此，相应的real cache会根据工作负载来增大或减小。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/AC-Key_F_3.png" alt="image-20211209175114565"></p><p>但是，像ARC这种基于页面的缓存算法都不很适合LSM- KVS，因为它们都基于「页面大小」和「缓存收益」一致的假设。而在LSM- KVS中，不同的key-values的「大小」以及「缓存它们所带来的收益」是不一样的，也即「缓存成本」和「缓存收益」不统一（缓存成本为缓存条目所占用的内存空间，缓存收益为缓存条目所能节省的磁盘I/O次数）。</p><p>因此，在LSM-KVS的缓存方案设计中，「缓存成本」和「缓存收益」应该和「访问频率」一块被考虑。</p><h2 id="0x20-Motivation"><a href="#0x20-Motivation" class="headerlink" title="0x20 Motivation"></a>0x20 Motivation</h2><h3 id="0x21-Unique-Challenges-in-Caching-for-LSM-面临的挑战"><a href="#0x21-Unique-Challenges-in-Caching-for-LSM-面临的挑战" class="headerlink" title="0x21 Unique Challenges in Caching for LSM (面临的挑战)"></a>0x21 Unique Challenges in Caching for LSM (面临的挑战)</h3><p>首先，与页面缓存替换问题相比，页面缓存替换问题中的页面大小是固定的，而LSM树中的key-values大小不是一致的。因此，在LSM中的缓存算法在设计替换算法时，应该将key-values大小的不一致性纳入考虑范围。</p><p>此外，LSM-KVS有两种截然不同的读取操作，也即point lookup和range query，对缓存的需求不一致，这也在设计LSM缓存算法时也带来了额外的挑战。</p><p>最后，LSM-KVS的Compaction和Flush操作会使缓存的条目失效，在设计缓存方案时也要进行特殊处理。</p><h3 id="0x22-What-to-Cache-in-LSM-KVS-缓存什么的问题"><a href="#0x22-What-to-Cache-in-LSM-KVS-缓存什么的问题" class="headerlink" title="0x22 What to Cache in LSM-KVS (缓存什么的问题)"></a>0x22 What to Cache in LSM-KVS (缓存什么的问题)</h3><p>将缓存KV和KP进行比较，命中KV能节省更多次数的磁盘I/O，因为命中KP仍然需要一次额外的磁盘I/O操作。但是，另一方面，当value过大时，缓存KP的成本更低，空间利用更高效。</p><p><strong>Lesson 1: 应该结合缓存KV和KP条目的优点，来更高效的服务「point lookup」。</strong></p><p>但是遗憾的是，缓存KV和KP都不能支持「range query」，因此LevelDB和RocksDB都选择缓存「data block」来为「range query」服务。</p><p><strong>Lesson 2: 缓存block或者KV/KP都有它们各自的优势来支持「range query」或「point lookup」。也即每种缓存条目都有其适用的工作负载。</strong></p><h3 id="0x23-How-to-Perform-Replacement-如何执行替换的问题"><a href="#0x23-How-to-Perform-Replacement-如何执行替换的问题" class="headerlink" title="0x23 How to Perform Replacement (如何执行替换的问题)"></a>0x23 How to Perform Replacement (如何执行替换的问题)</h3><p>从上文的讨论可知，每种缓存条目都有其适应的工作负载场景。表1列出了各种缓存条目的比较。但是设计一个由三种缓存条目组成的替换算法是极具挑战的。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/AC-Key_T_1.png" alt="image-20211209183758144"></p><p>现有的成本感知缓存方案（cost-aware caching schemes）并没有针对LSM-KVS的场景做缓存成本和收益的分析。</p><p><strong>Lesson 3: 缓存算法应该根据LSM-KVS独特的分层结构，考虑不同缓存条目所「占用DRAM空间大小的不同」，以及「节省的I/O次数的不同」。</strong></p><p>通常，现有缓存算法会为各种缓存条目设置固定大小的缓存空间，但这会带来一系列问题。首先，很难设置最优的缓存空间分配；其次，即使一开始设计好了最优的缓存空间分配，但随着工作负载发生变化，之前设置的缓存空间分配便不再适用。</p><p><strong>Lesson 4：缓存算法应该能够自适应工作负载的变化。</strong></p><h2 id="0x30-Technique-AC-Key-Design"><a href="#0x30-Technique-AC-Key-Design" class="headerlink" title="0x30 Technique: AC-Key Design"></a>0x30 Technique: AC-Key Design</h2><p>AC-Key同时支持缓存三种类型（KV，KP和Block）的条目，并为它们设计了单独的缓存组件。此外，各个缓存组件的空间大小会根据「分层自适应缓存算法」（<em>hierarchical adaptive caching</em> algorithm，HAC）来调整，该算法考虑了不同缓存条目的异构成本和收益，并使用一个「缓存效率因子」（<em>caching efficiency factor</em>）来指导缓存组件大小的调整。</p><h3 id="0x31-AC-Key-Caching-Components"><a href="#0x31-AC-Key-Caching-Components" class="headerlink" title="0x31 AC-Key Caching Components"></a>0x31 AC-Key Caching Components</h3><p>AC-Key的系统架构如图4所示。Block，KP和KV缓存，通过E-LRU进行管理，E-LRU是一种改进的LRU（基于缓存效率因子来选择缓存条目进行逐或者说是替换）。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/AC-Key_F_4.png" alt="image-20211209191340705"></p><p>在一个point lookup中，如果一个lookup-key是首次被访问，则会先将它存到KP Cache中。缓存到KP Cache中的key被称为「warm key」。如果KP Cache中的一个「warm key」被再次命中，我们会将其视为一个「hot key」。我们预计将来访问它的可能性更高。因此，我们会将其提升到KV Cache中，来为将来的访问减少潜在的磁盘I/O次数。</p><p>一种优化方式是，如果KV对比KP对还要小，我们会将key-value缓存到<strong>KP Cache</strong>中，而不是选择将KP缓存到KP Cache中，这样既能避免额外的磁盘I/O，还能节省缓存空间。不过这个KV对仍然需要再次命中后才会被移动到KV Cache中。</p><h4 id="a-Get-Handling"><a href="#a-Get-Handling" class="headerlink" title="a. Get Handling"></a>a. Get Handling</h4><p>查询一个Store中存在的Key时（也即point lookup操作）。首先会查询MemTable，因为MemTable中可能存在最新版本的Value。如果没有在MemTable中查询到，则会在KV和KP缓存中查询，将会发生以下情形：</p><ul><li>Case I：命中KV Cache。不需要磁盘I/O，直接返回相应Value值即可；</li><li>Case II：没有命中KV Cache，但是命中KP Cache。此时，先检查指针指向的数据块是否被缓存到Block Cache中，如果没有则将此数据块加载到Block Cache中，然后在该数据块中使用二分查找定位KV对。此外，还需要将此Key从KP Cache提升到KV Cache中；</li><li>Case III：既没有命中KV Cache，也没有命中KP Cache。将会一个Level一个Level的查询每个有序run，查找完成后将其缓存到KP Cache中（需要注意的是，在检索SST时会使用到BF block以及Index block，如果它们不在Block Cache中，也要将它们缓存到Block Cache）；</li></ul><h4 id="b-Flush-Handling"><a href="#b-Flush-Handling" class="headerlink" title="b. Flush Handling"></a>b. Flush Handling</h4><p>Flush会将包含最新KV数据的MemTable写入到磁盘组件$L_0$中。需要注意的是，MemTable中的某个Key的旧数据此前可能已经缓存到KV或KP Cache中。如果包含最新数据的MemTable还没有Flush到磁盘上，则在执行<code>Get</code>时会先检查MemTable，KV或KP Cache中的旧数据不会对<code>Get</code>造成影响，但是如果将MemTable Flush到磁盘上，则在执行<code>Get</code>操作时可能得到的是缓存中的旧版本的数据。因此，必须在Flush前完成MemTable和缓存中重叠Key的同步（只要有缓存就有可能存在缓存不一致的问题）。</p><p>有两个可以进行同步的时间点，一种是在<code>Put</code>期间同步，一种是在<code>Flush</code>期间同步。</p><p>如果在<code>Put</code>期间进行同步，则在每次<code>Put</code>操作时都需要检查缓存中的条目，会带来额外的开销；此外，在此期间无法更新KP Cache，因为MemTable还没写到SST中，无法获得指针。</p><p>因此，AC-Key选择仅在<code>Flush</code>时进行同步，这时只需要同步一次，并且可以在此时计算出指针地址来更新KP缓存。</p><h4 id="c-Compaction-Handling"><a href="#c-Compaction-Handling" class="headerlink" title="c. Compaction Handling"></a>c. Compaction Handling</h4><p>Compaction操作会影响KP和Block Cache。因为此操作会删除旧的SST，而旧的SST中的数据可能已经缓存到了KP或Block Cache中。需要注意的是此操作不会影响KV Cache，因为<code>Compaction</code>操作不会修改数据，只会进行重排和合并。</p><p>AC-Key会更新受到到<code>Compaction</code>影响的KP或Block Cache。</p><h3 id="0x32-Caching-Efficiency-Factor"><a href="#0x32-Caching-Efficiency-Factor" class="headerlink" title="0x32 Caching Efficiency Factor"></a>0x32 Caching Efficiency Factor</h3><p>为了对缓存条目的成本和收益进行定量分析和权衡，本文根据LSM-KVS独特的分层结构提出了一个「缓存效率因子」。使用此缓存效率因子，AC-Key将LRU改进为E-LRU来管理每个缓存组件内部的替换操作。此外，AC-Key还通过这个缓存效率因子将ARC改进为E-ARC来调整各个缓存组件的大小。</p><p>本文将一个缓存条目的缓存效率因子$E$（$E$代表Efficiency）定义为如下等式：</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/image-20211209202701009.png" alt="image-20211209202701009"></p><p>其含义是每字节的DRAM空间所节省的I/O次数。</p><p>其中，$b$代表如果此条目被缓存，其潜在所能节省的I/O次数，它由如下等式给出：</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/AC-Key_E_2.png" alt="image-20211209203050655"></p><p>函数式$f(m)$依赖于LSM-KVS的实现，通常$f(m) = m+2$，也即需要读$m个BF + 1个index\ block + 1个data \ block$    。</p><p>传统的LRU仅考虑了访问模式，而没有考虑缓存条目的收益和成本不同。将LRU与缓存效率因子结合所得的E-LRU便是为了解决这一问题。E-LRU检查最少使用的a个缓存条目，并从中选出缓存效率因子最小的那个条目进行替换。$a$的值取决于缓存条目的缓存效率因子$E$的方差，它由$a= e^{v}$给出，其中$v$是缓存条目的缓存效率因子$E$的标准偏差。</p><p>当$v=0$时，所有缓存条目具有相同的缓存效率，此时$a=1$，E-LRU退化为LRU。此外，$a$有一个上限值，以避免在做出替换决定时需要检查过多的条目。</p><h3 id="0x33-HAC-Hierarchical-Adaptive-Caching"><a href="#0x33-HAC-Hierarchical-Adaptive-Caching" class="headerlink" title="0x33 HAC: Hierarchical Adaptive Caching"></a>0x33 HAC: Hierarchical Adaptive Caching</h3><p>Hierarchical Adaptive Caching (HAC)具有两级层次结构来管理不同的缓存（如图5所示）。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/AC-Key_F_5.png" alt="image-20211209211404375"></p><p>在上层，缓存被分为两个组件：<code>Point Cache</code>以及<code>Block Cache</code>，两个组件之间的边界可以动态调整。</p><p>在下层，<code>Point Cache</code>被进一步分为<code>KV Cache</code> 和<code>KP Cache</code>，同样也有一个动态调整的边界。</p><p>HAC通过维持<code>ghost caches</code>来保存从<code>KV Cache</code>、<code>KP Cache</code>和<code>Block Cache</code>中替换出来条目的记录。在上层和下层各有两个<code>ghost cache</code>。与<code>ghost cache</code>相对应，原始的<code>KV Cache</code>、<code>KP Cache</code>和<code>Block Cache</code>被称为<code>real cache</code>。在这里，<code>KV Real Cache</code>和<code>KP Real Cache</code>组成<code>Point Real Cache</code>。</p><p><code>ghost cache</code>并不保存真正的条目，只保存相关条目的元数据。一个<code>ghost cache</code>的命中意味着本应该命中对应的<code>real cache</code>的，如果<code>real cache</code>足够大的话。通过使用<code>ghost cache</code>和缓存效率因子，文章设计了E-ARC来调整相应的<code>real cache</code>的大小。</p><h4 id="a-Lower-Level-HAC"><a href="#a-Lower-Level-HAC" class="headerlink" title="a. Lower-Level HAC"></a>a. Lower-Level HAC</h4><p>在下层HAC，<code>Point Cache</code>被划分为<code>KV Real Cache</code>($R_{KV}$)和<code>KP Real Cache</code>（$R_{KP}$），这里有<br>$$<br>\left | R_{KV} \right |+\left | R_{KP} \right | = \left | S_{point} \right |<br>$$<br>其中$\left | S_{point} \right |$表示<code>Point Cache</code>的大小。AC-Key维护<code>KV Ghost Cache</code>，就好像$R_{KV}$ 加上$G_{KV}$的缓存大小就等于整个<code>Point Cache</code>的大小（这是用于暗示$R_{KV}$最大可以扩展到整个<code>Point Cache</code>的大小，而此时相应的$R_{KP}$的大小为0）。因此，在这个前提下，下面的等式成立：<br>$$<br>\left | S_{point} \right |=\left | R_{KV} \right |+\left | R_{KP} \right | = \left | R_{KV} \right |+\left | G_{KV} \right |=\left | R_{KP} \right |+\left | G_{KP} \right | \ \ \ Eqn.4<br>$$<br>下面展示E- ARC处理缓存命中和未命中的几种情况：</p><ul><li>Case I：命中<code>Real Cache</code>。缓存命中$R_{KV}$或$R_{KP}$，将该条目移动到$R_{KV}$的MRU端。特殊的，如果命中发生在$R_{KP}$则需要一次额外的磁盘I/O来获取<code>Value</code>，然后将该<code>key-value</code>对提升到$R_{KP}$中。</li><li>Case II：命中<code>KV Ghost Cache</code>。这意味着$R_{KV}$应该更大，因此向<code>KP Cache</code>方向调整目标边界，调整步幅为$\delta = kE$，$E$代表$G_{KV}$中命中条目的缓存效率因子，$k$是配置的学习率。从磁盘中读完数据后将其插入到$R_{KV}$的MRU端。为了给该条目腾出空间，如果目标边界在$R_{KV}$内部的话，这意味着<code>KV Cache</code>的目标大小比当前大小还要小，不能扩充，只能在<code>KV Cache</code>内部通过E-LRU算法进行替换。</li><li>Case III：命中<code>KP Ghost Cache</code>。这意味着$R_{KP}$应该更大，因此调整目标边界，同Case II类似。</li><li>Case III：未命中缓存。此时需要检索磁盘，然后将读取的数据缓按照KP的格式存到$R_{KP}$中。此外，$R_{KP}$同样需要为该条目腾空子，需要根据目标边界的实际情况来确定是调整$R_{KP}$的大小来腾空子还是通过缓存替换来腾空子。</li></ul><p>$R_{KV}$和$R_{KP}$之间的目标边界指示着实际边界应该移动的方向，但是需要注意的是，实际边界总是滞后于目标边界的。</p><p>高层的操作顺序如下：</p><ol><li>命中<code>ghost cache</code>则调整目标边界；</li><li>需要缓存的条目的插入或提升（从<code>KP Cache</code>提升到<code>KV Cache</code>）使得实际边界向目标边界方向调整，相应的，$R_{KV}$和$R_{KP}$的大小得到更新；</li><li>基于新的<code>Real Cache</code>的大小和Eqn.4（上文的等式4）来调整对应的<code>Ghost Cache</code>的大小；</li><li><code>Real Cache</code>和<code>Ghost Cache</code>在需要适配更新后的大小时通过E-LRU算法来执行替换；</li></ol><h4 id="b-Upper-Level-HAC"><a href="#b-Upper-Level-HAC" class="headerlink" title="b. Upper-Level HAC"></a>b. Upper-Level HAC</h4><p>在HAC的上层，我们重新应用E-ARC来调整<code>Point Cache</code>和<code>Block Cache</code>之间的边界。<code>Block Cache</code>和<code>Point Cache</code>各自都有一个 <code>Real Cache</code>和一个<code>Ghost Cache</code>。需要注意的是，从$R_{point}$中替换出来的条目不光要插入到$G_{point}$还要插入到下层的$G_{KV}$或$G_{KP}$中。与下层相似，上层的Cache空间也存在类似的等价关系：<br>$$<br>\left | S_{total} \right |=\left | R_{point} \right |+\left | R_{block} \right | = \left | R_{block} \right |+\left | G_{block} \right |=\left | R_{point} \right |+\left | G_{point} \right | \ \ Eqn.5<br>$$<br><strong>目标边界的调整：</strong></p><p>命中$G_{block}$需要将目标边界从$R_{block}$移向$R_{point}$，移动步幅$\Delta = kE$。相应的，$R_{point}$的目标大小将会减少$\Delta$。在下层，调整量将按照$R_{KV}$和$R_{KP}$的当前目标大小的比率按比例分配。例如，当前$R_{KV}$和$R_{KP}$的目标大小分别为$\left | R_{KV}^* \right |$和$\left | R_{KP}^* \right |$，则它们将按照如下方式进行更新：<br>$$<br>\begin{matrix}<br>\left | R_{KV}^* \right | \leftarrow \left | R_{KV}^* \right | - \Delta \frac{\left | R_{KV}^* \right |}{\left | R_{KV}^* \right | + \left | R_{KP}^* \right |}<br>\\<br>\left | R_{KP}^* \right | \leftarrow \left | R_{KP}^* \right | - \Delta \frac{\left | R_{KP}^* \right |}{\left | R_{KV}^* \right | + \left | R_{KP}^* \right |}<br>\end{matrix}<br>$$<br>另一方面，如果命中的是$G_{point}$，则需要将目标边界向$R_{block}$方向移动，移动步幅同样为$\Delta = kE$。需要注意的是，这里的$E$会比命中$G_{block}$时的缓存效率因子更大，因为与缓存Block相比，缓存KV和KP在占用相同空间的情况能节省更多的I/O次数。此外，在HAC的上层命中$G_{point}$则意味着会在下层命中$G_{KV}$或$G_{KP}$，在这种情况下会先调整下层的目标边界，再调整上层的目标边界。</p><p><strong>实际边界的调整：</strong></p><p>当一个block需要插入到<code>Block Cache</code>中（也即<code>Block Cache</code>潜在需要更大的空间时），而<code>Block Cache</code>的实际大小加上此block的大小比其目标大小要大时，<code>Block Cache</code>不会扩容，只能通过E-LRU算法从<code>Block Cache</code>中选择一个block进行替换。而如果<code>Block Cache</code>的实际大小加上此block的大小要比其目标大小要小时，此时就可以对<code>Block Cache</code>进行扩容了（相对应的<code>Point Cache</code>会收缩缓存空间），扩容后就可以将此block插入到<code>Block Cache</code>。</p><p>另一方面，如果<code>Point Cache</code>需要更大的容量，HAC会估计其进行扩容后的大小，然后将该大小与其目标大小进行比较，如果小于其目标大小的话，则可以对其进行扩容，相应的，<code>Block Cache</code>需要通过逐出block来收缩缓存空间以给Point Cache提供空间；如果预估的新的<code>Point Cache</code>的大小要大于其目标大小的话，则不能对其进行扩容，只能通过E-LRU算法在<code>Point Cache</code>内部通过替换来为新的缓存条目腾出空间。</p><h4 id="c-Reduce-Ghost-Cache-Size"><a href="#c-Reduce-Ghost-Cache-Size" class="headerlink" title="c. Reduce Ghost Cache Size"></a>c. Reduce Ghost Cache Size</h4><p>在ARC算法的设计中，从<code>Real Cache</code>中逐出的页面，其内容会被丢弃，仅需要将<code>page number</code>保存在<code>Ghost Cache</code>中。这里，<code>page number</code>的大小与<code>page content</code>相比是可以忽略不计的。</p><p>类似的，在AC-Key中<code>Ghost Block  Cache</code>的大小与<code>Real Block Cache</code>的大小相比也是可以忽略的，因为<code>Ghost Block Cache</code>中以<code>&lt;SstID | BlockOffset&gt;</code>形式存储的<code>block handle</code>与<code>Real Block Cache</code>中存储的Block相比也是可以忽略的。但是<code>Ghost KV Cache</code>、<code>Ghost KP Cache</code>以及<code>Ghost Point Cache</code>的大小与其对应的<code>Real Cache</code>相比是无法忽略的，这会带来较大的空间开销。  </p><p>AC-Key通过两种方式来减少<code>Ghost Cache</code>的空间开销：</p><ul><li>首先，AC- Key不使用从<code>KV Cache</code>和<code>KP Cache</code>中逐出来的条目的原始Key，而是使用原始Key的哈希值；</li><li>其次，可以在自适应缓存方案已经成功建立了一个在<code>KV Cache</code>、<code>KP Cache</code>和<code>Block Cache</code>之间比较有利的空间分配时关闭掉<code>Ghost Cache</code>，来消除其带来的开销。此外，还可以根据当前缓存组件的命中率来判断是否需要打开<code>Ghost Cache</code>以重新指导空间分配；</li></ul>]]></content>
    
    
    <categories>
      
      <category>Key-Value Store</category>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>Key-Value Store</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TRIAD——Creating Synergies Between Memory, Disk and Log in Log Structured Key-Value Stores</title>
    <link href="/2021/12/09/TRIAD%EF%BC%9ACreating%20Synergies%20Between%20Memory,%20Disk%20and%20Log%20in%20Log%20Structured%20Key-Value%20Stores/"/>
    <url>/2021/12/09/TRIAD%EF%BC%9ACreating%20Synergies%20Between%20Memory,%20Disk%20and%20Log%20in%20Log%20Structured%20Key-Value%20Stores/</url>
    
    <content type="html"><![CDATA[<h2 id="0x00-Introduction-amp-Background"><a href="#0x00-Introduction-amp-Background" class="headerlink" title="0x00 Introduction &amp; Background"></a>0x00 Introduction &amp; Background</h2><p>LSM通常由两部分组件组成：</p><ul><li>内存组件（Memory Component）：主要用来absorb update（在内存组件中absorb update的同时会将数据写入到磁盘日志中，以用于数据恢复）；</li><li>磁盘组件（Disk Component）：用于持久化数据，内存组件中的数据超过上限后会flush到磁盘，磁盘组件按照Level的形式进行组织，接近内存组件的Level保存着更新的数据，当某个$L_i$超过上限时会与$L_{i+1}$进行归并压缩，也即在后台执行$Compaction$操作；</li></ul><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/TRIAD_1.png" alt="image-20211208161344551"></p><p>$Compaction$和$Flush$是非常关键的操作，负责维护$LSM$的结构和属性，同时这也会占用大量的可用资源。</p><p>作者在$Nutanix$的生产环境负载测试中，顶峰时$Compaction$操作对$CPU$的占用率会达到45%。此外，单个集群平均每天要花费两个小时在$Compaction$操作上，以维护元数据存储映射。</p><p>文章提出了三种新的互补技术来弥补这些不足。这些技术同时减少了$Compaction$和$Flush$操作所占用的时间和空间资源，并带来了吞吐量的提升。</p><ol><li>第一项技术减少了倾斜工作负载（skewed workload）下的$Compaction$开销。将频繁更新的$KV$对（也即hot entries）保留在内存组件中，仅对cold entries执行$Flush$操作。这个分离操作减少了hot entry所触发的频繁Compaction操作。</li><li>第二项技术的主要思想是推迟对文件的Compaction操作，只有当文件的重叠部分足够大时才进行Compaction。</li><li>第三项技术主要是改变提交日志（commit log）在LSM中的角色，以类似SSTable的形式来使用它们，来避免双写（第一次写到日志，第二次是flush操作所引起的第二次写操作）；</li></ol><h2 id="0x10-Motivation"><a href="#0x10-Motivation" class="headerlink" title="0x10 Motivation"></a>0x10 Motivation</h2><p>尽管I/O操作不在面向用户操作的关键路径中，但是Flush、Compaction、Log等操作仍然会占用大量的计算资源。CPU协调这些操作会占用大量的处理能力，而这些处理能力本来是可以用于为用户提供更好性能体验的。因此I/O操作的频率和长度会对用户感知的最终性能有较大的影响。</p><p>为证明I/O操作会导致性能下降，本文设计了相关实验进行验证。选择了两个方面具有负载倾斜的工作负载：</p><ul><li>数据流行度：skewed和uniform</li><li>读写混合：write dominated 和 balanced</li></ul><p>将上述工作负载运行在普通RocksDB和关闭了后台I/O的RocksDB（例如，可以关闭$Flush$和$Compaction$）中，实验结果绘图如下：</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/TRIAD_2.png" alt="image-20211208171104923"></p><p>可以看到在各个负载下，关闭了后台I/O的RocksDB都表现出了更优的性能。在此结果的驱动下，本文确定了触发频繁和密集I/O的三个主要原因：</p><ul><li> data-skew unawareness, at the memory component level，也即在内存组件层面上的无意识的数据倾斜（存在冷热键）;</li><li> premature and iterative compaction, at the LSM tree level，也即在LSM树层面过早的迭代$Compaction$操作;</li><li>duplicated writes at the logging level，也即在日志层面的双写操作； </li></ul><h3 id="0x11-Data-skew-unawareness-（数据倾斜）"><a href="#0x11-Data-skew-unawareness-（数据倾斜）" class="headerlink" title="0x11 Data skew unawareness （数据倾斜）"></a>0x11 Data skew unawareness （数据倾斜）</h3><p>很多KV Store的工作负载都表现出数据倾斜的特征，其中hot key的更新频率要远远高于cold key。</p><p>数据倾斜导致commit log的增长速度远远大于$C_m$（也即内存中的MemTable和Immutable Memtable）。因为对相同key的更新，日志是以追加的方式存储的，而$C_m$采取的是就地吸收（absorbed in-place）的方式。这会导致$C_m$在达到其上限之前触发$Flush$操作，因为commit log过大会导致在数据恢复时所耗费的时间更长，所以MemTable必须频繁的$Flush$，从而使commit log可以被及时清理掉，通过减小commit log的大小来缩短数据恢复的时间。这不仅会增加$C_m$的$Flush$频率，而且由于其大小是小于其能达到的最大值的，导致$L_0$中打开和存储文件的固定成本并没有被实际写入数据更好的摊分。</p><h3 id="0x12-Premature-and-iterative-compaction-（过早Compaction）"><a href="#0x12-Premature-and-iterative-compaction-（过早Compaction）" class="headerlink" title="0x12 Premature and iterative compaction （过早Compaction）"></a>0x12 Premature and iterative compaction （过早Compaction）</h3><p>现有的基于LSM的KV-Store在Compaction过程中表现出了两方面的限制。</p><p>一些KV-store在$L_0$中仅保留一个SSTable，以加快读速率。但这样会导致每次内存组件的Flush操作都会触发一次$L_0$和底层Level的Compaction操作，也即导致了频繁的Compaction操作。这便是第一种限制。</p><p>另外一些LSM的方案是在$L_0$保存多个SSTable，这会导致第二种限制。主要表现在，当$L_0$中有多个SSTable时，LSM如何将$L_0$ Compaction到$L_1$。事实上，$L_0$中的多个文件是一次性Compaction到更高的Level的，这从而也会导致多个连续的Compaction操作。也即如果$L_0$中的两个SSTable都有一个相同的key，那么这个key将会在底层的LSM树中执行两次Compaction操作。而数据倾斜会加剧这个问题，因为它增加了$L_0$中多个SSTable拥有共同key的概率。显然，当系统的负载越高时，这种事件发生的概率也会越高。</p><h3 id="0x13-Duplicate-writes（双写）"><a href="#0x13-Duplicate-writes（双写）" class="headerlink" title="0x13 Duplicate writes（双写）"></a>0x13 Duplicate writes（双写）</h3><p>当$C_m$被Flush到$L_0$中后，相应的commit log便会被删除掉，因为$Flush$操作已经能够保证数据的持久化。Flush到$L_0$的每个KV对都是最新的版本，都对应了一个commit log，因此$Flush$操作不过是重放了在追加commit log时已经执行过的I/O操作。</p><h2 id="0x20-Technique——TRIAD"><a href="#0x20-Technique——TRIAD" class="headerlink" title="0x20 Technique——TRIAD"></a>0x20 Technique——TRIAD</h2><p>文章用来解决I/O开销的方法体现在三个方面，每个解决方案都对应了前面提到的一个挑战：</p><ul><li> TRIAD-MEM 通过冷热键分离来解决内存组件中的data skew问题；</li><li>TRIAD- DISK 通过推迟和批量Compaction来解决磁盘组件中的premature and iterative compaction问题；</li><li>TRIAD- LOG 通过绕过$Flush$过程中创建新SSTable来解决日志层面中的duplicate write问题；</li></ul><h3 id="0x21-TRIAD-MEM"><a href="#0x21-TRIAD-MEM" class="headerlink" title="0x21 TRIAD-MEM"></a>0x21 TRIAD-MEM</h3><p>TRIAD-MEM的目标是利用大多数工作负载所表现出的数据倾斜的特点来减少Flush，并进一步减少Compaction的频率。为了做到这一点，TRIAD-MEM仅将cold key flush到磁盘上，而将hot key保留在内存中（但也要写commit log）。这能避免触发大量的Compaction操作，从而降低磁盘中重叠Key的数量。</p><p>将hot key保留在内存中，便可以在内存中完成更新操作，这可以避免在磁盘上触发大量的Compaction操作。</p><p>图3展示了Flush期间冷热键分离的过程，图4展示了Flush完成后的状态。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/TRIAD_3_4.png" alt="image-20211208204615405"></p><p>算法2展示了冷热键是如何分离的。旧$C_m$中的前K个条目被选中，其中K是系统的参数。理想情况下，K的取值应该既能保存尽量多的hot key，还能避免引起较高的内存开销。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/TRIAD_A_2.png" alt="image-20211208205815580"></p><p>保留在内存中的hot key 不需要Flush到磁盘上，但在更新时仍然需要写commit log（如图3所示），以保证在宕机时能完成数据恢复。</p><p>当$C_m$的大小不超过某个阈值（算法1中的<code>FLUSH_TH</code>）时，分离冷热键时不会进行$Flush$。因为在工作负载不均衡时，$Flush$ 可能是因为commit log满了而不是$C_m$满了所触发的（commit log太多会造成数据恢复时间过长，所以commit log过多时也会触发$Flush$）。所以，为了避免$Flush$小的$C_m$文件，算法会将所有条目都保存在内存中，并根据这些条目创建新的commit log，丢弃旧的commit log。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/TRIAD_A_1.png" alt="image-20211208211005386"></p><h3 id="0x22-TRIAD-DISK"><a href="#0x22-TRIAD-DISK" class="headerlink" title="0x22 TRIAD-DISK"></a>0x22 TRIAD-DISK</h3><p>TRIAD-DISK会推迟Compaction操作，直到需要Compaction的文件有足够多的key重叠。为了估计文件之间的key重叠，文章使用了HyperLogLog (HLL)概率基数估计器。</p><p>为了计算一组文件之间的重叠，文章定义了一个重叠率（overlap ratio），假设在$L_0$中有$n$个文件，则重叠率可以定义为：<br>$$<br>1-\frac{ UniqueKeys(file_1, file_2,… ,file_n))}{sum(Keys(file_i))}<br>$$<br>其中：</p><ul><li>$Keys(file_i)$表示第$i$个SSTable中的key数量；</li><li>$UniqueKeys(file_1, file_2,… ,file_n))$表示合并$n$个文件后的唯一key的数量；</li></ul><p>图5展示了overlap ratio 是如何来推迟Compaction的。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/TRIAD_F_5.png" alt="image-20211208212910135"></p><p>在$L_0$中执行每次Compaction之前，我们会先计算出$L_0$中所有文件的overlap ratio，如果overlap ratio小于阈值，则推迟Compaction（除非$L_0$中文件的数量超过允许的最大值，如果超过，那么不再考虑overlap ratio，强制允许Compaction）。</p><h3 id="0x23-TRIAD-LOG"><a href="#0x23-TRIAD-LOG" class="headerlink" title="0x23 TRIAD-LOG"></a>0x23 TRIAD-LOG</h3><p>TRIAD-LOG的主要思路是，内存中需要Flush到磁盘中的数据已经在commit log中了，可以对commit log进行利用而不必通过Flush操作再重写一次。主要思想就是将CL转为$L_0$中一种特殊的SSTable，也即CL-SSTable，这种方式可以避免完整的Flush操作。</p><p>TRIAD-LOG增强了commit log角色的能力。当向$C_m$写入数据时，commit log就扮演其普通WAL的角色。当触发Flush操作时，将commit log转为CL-SSTable而不是将$C_m$重写到$L_0$。</p><p>将commit log作为$L_0$中的SSTable可以避免Flush操作所带来的I/O开销，但SSTable是有序的，而commit log是追加生成的、是无序的。SSTable有序的特性可以加速Compaction操作和检索操作。</p><p>为了避免在$L_0$中查找某个条目时需要扫描整个CL-SSTable，TRIAD-LOG在$C_m$中保存每个KV对应的最近更新的commit log文件的偏移量，以帮助构建有序的CL-SSTable。一旦Flush操作被触发，只需要将与最新的commit log偏移量相关联的索引写到磁盘上（索引也即index的大小与整个KV相比是很小的，所以仅刷新index的开销很小，文章并没有对index有明确说明，我觉的这里的index可以是<code>(key, CL name, offset)</code>的组合，通过这个index再加上CL就可以组成有序的CL-SSTable了）。</p><p>TRIAD-LOG并没有改变写操作的流程，不同之处仅在于，在$C_m$中执行更新时会把对应的commit log的偏移量和文件名也写入到$C_m$中（例如，<code>(key, value, offset, CL name)</code>）。其流程入下图所示。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/TRIAD_F_6.png" alt="image-20211209093221560"></p>]]></content>
    
    
    <categories>
      
      <category>Key-Value Store</category>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>Key-Value Store</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务实现算法</title>
    <link href="/2021/10/20/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <url>/2021/10/20/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="私有工作空间"><a href="#私有工作空间" class="headerlink" title="私有工作空间"></a>私有工作空间</h2><p>在概念上，当一个<strong>进程</strong>开始一个事务时，它被分配一个私有工作空间，该工作空间包含所有它有权访问的文件。在事务提交或中止前，它的所有读写操作都在私有空间内进行，而不是直接对文件系统进行操作。这就直接导致了第一种实现方法的产生；在进程开始一个事务的时刻，实际上为该进程分配了一个私有工作空间。</p><p>此技术带来的问题是把所有的东西都<strong>复制到私有工作空间的开销</strong>是十分大的，但是各种各样的优化方法使这种方法可行。</p><p>第一种优化方法是基于这样的认识，即<strong>当一个进程只读取一个文件而不对它做修改时，就不需要私有拷贝</strong>。该进程可以直接使用真正的文件（除非自事务开始以来文件已被改动）。因此，当进程开始一个事务时，就为它创建一个私有的工作空间，该空间是空的，除非当一个指针回指到它的父辈工作空间。当<strong>进程为了读取而打开文件时，指针将回指，直到可以在父辈（或者更老的祖先）工作空间中找到文件为止。</strong></p><p>第二种优化方法可以大大减少复制工作量。因为它不是复制整个文件，而是<strong>只将索引复制到私有工作空间</strong>。索引是与判断文件所在磁盘块位置有关的数据块。在UNIX系统中，索引是$i$节点（index node，即索引节点）。通过<strong>私有索引</strong>，文件可以按通常方式读取，这是因为索引中所包含的地址指向的是文件所在的原始磁盘块。然而，<strong>当一个文件块第一次被修改时，将生成该块的副本，其地址也被插入索引中</strong>（如下图所示）。然后就可以在不影响原始块的情况下更新这个块。添加块也是用这种方法解决。新块有时会被称为影像块。（这是一种写时复制技术，即copy-on- write）</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/%E5%86%99%E6%97%B6%E5%A4%8D%E5%88%B6.drawio.png" alt="写时复制.drawio"></p><p>在上图中，运行事务的进程看到了修改的文件，但是其他所有进程看到的仍是原始的文件。在更复杂的事务中，私有工作空间可能包含大量的文件而不仅仅是一个。<strong>如果事务中止，私有工作空间就被简单的删除，它所指向的私有块也将被释放回自由列表（free list）中。如果事务被提交了，那么私有索引被移到父辈工作空间中。</strong></p><h2 id="写前日志"><a href="#写前日志" class="headerlink" title="写前日志"></a>写前日志</h2><p>另一个实现事务的常用方法是写前日志（write ahead log，WAL）。使用这种方法时，<strong>文件将真正被修改，但是在任何一个数据块被修改前，一条记录被写到了日志中以说明哪个事务正在对文件进行修改，哪个文件和哪个数据块被改动了，旧值和新值是什么。只有当日志被写入成功后，此改动才可以被写入文件。</strong></p><p>如果事务执行成功并被提交，那么一条提交记录被写进日志，但是数据结构不需要变动，这是因为它们已经被更新了。如果事务中止，那么可以使用日志来回退到原来的状态。<strong>从日志的末尾开始向前读取每条记录，同时将在每条记录中描述的改动撤销</strong>（这也被称为「回退」或「回滚」，也即rollback）。</p><p>注：本文内容摘自「分布式系统原理与范型」，供本人学习回顾使用。</p>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式系统原理与范型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>并发控制</title>
    <link href="/2021/10/20/%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/"/>
    <url>/2021/10/20/%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>并发控制算法通常根据<strong>读写操作同步的方式</strong>来分类。同步可以通过<strong>共享数据上的互斥机制</strong>（例如，锁🔒），或者通过显式地使用<strong>时间戳排序</strong>来实现。</p><p>并发控制算法可以进一步区分为<strong>悲观算法</strong>和<strong>乐观算法</strong>。</p><p>悲观算法（pessimistic approaches）的基本原则是Murphy定律（墨菲定律）：<strong>如果某事物可以出错，那么它就会出错。在悲观算法中，操作时在它们被执行前同步的，这意味着冲突在允许发生之前就解决了</strong>。</p><p>相反，乐观算法（optimistic approachs）是<strong>基于错误一般不会发生的观点</strong>。所以操作被简单地执行，在事务结束的时候再进行同步。如果那是确实发生了冲突，一个或更多的事务将被迫终止。</p><p>下面将讨论两个悲观算法和一个乐观算法。</p><h2 id="两阶段锁定"><a href="#两阶段锁定" class="headerlink" title="两阶段锁定"></a>两阶段锁定</h2><p>最古老最简单也是最广泛使用的并发控制算法是锁定（locking🔒）。</p><p>当一个进程要作为事务的一部分来读或写一个数据项时，它请求调度管理器允许它给该数据项加锁。同样，当它不再需要一个数据项时，就请求调度管理器释放该锁。</p><p><strong>调度管理器的任务</strong>是以一种可以得到正确调度结果的方式来允许加锁和释放锁，即需要使用一种<strong>可以提供串行调度的算法</strong>。这样一个算法是两阶段锁定算法。</p><p>如下图所示的两阶段锁定（two-phase locking，2PL）中，调度管理器先在增长阶段（growing phase）获得它所需要的所有锁，然后在收缩阶段（shrinking phase）释放它们。尤其要遵守下面三个规则：</p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/2PL.png" alt="17105307_MZJc" style="zoom: 50%;" /><ol><li>当调度管理器收到来自事务管理器的<code>open(T,x)</code>操作时，它检测该操作是否跟它已经允许锁定的另一个进程冲突。如果存在冲突，操作<code>open(T,x)</code>被延迟（这样事务$T$也被延迟）。如果没有冲突，调度管理器就允许对数据项$x$加锁，并将这个操作传递给数据管理器。</li><li>直到数据管理器通知它已经完成了对锁定数据项x的操作，调度管理器才会释放数据项x的锁。</li><li>一旦调度管理器为事务$T$释放了锁，那么无论事务T请求为哪个数据项加锁，调度管理器都不会允许T加另外一把锁（即只能一次获取到所需的所有锁）。$T$获取另外一把锁的任何企图都是一个程序错误，都会终止事务$T$。</li></ol><p>在许多系统中，收缩阶段是在事务运行结束之后，要么提交、要么终止时出现，它导致锁的释放（<strong>同时释放</strong>），如下图所示。这种策略被称为严格的两阶段锁定。</p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/strict-2PL.png" alt="17105326_2MZl" style="zoom:50%;" /><p>严格的两阶段锁定有两个主要的优点：</p><ul><li>事务总是读提交事务写入的值，所以我们从来都不会因为事务的计算是基于一个它不应该看到的数据项而终止它。</li><li>所有锁的获得和释放都可以由系统来处理而无需事务关心。要访问一个数据项就要获得锁，当事务完成时就释放锁。这种策略消除了瀑布型终止（cascaded aborts），即不得不撤销一个已经提交的事务，因为它看到了它不该看见的数据项。</li></ul><h2 id="悲观的时间戳排序"><a href="#悲观的时间戳排序" class="headerlink" title="悲观的时间戳排序"></a>悲观的时间戳排序</h2><p>一个完全不同的并发控制方法是在<strong>每个事务$T$开始时给它分配一个时间戳$ts(T)$。</strong></p><p>使用Lamport算法，我们可以保证时间戳是惟一的。<strong>事务$T$的每个操作都被盖上时间戳$ts(T)$，并且系统中的每个数据项$x$都有一个相关的读时间戳$ts_{RD}(x)$和写时间戳$ts_{WR}(x)$。</strong>读时间戳被设置为<strong>最近</strong>读$x$的事务的时间戳，而写时间戳是<strong>最近</strong>修改$x$的事务的时间戳。使用时间戳排序，如果两个操作冲突，则数据管理器先处理时间戳最早的操作。</p><p>现在假设调度管理器从具有时间戳$ts$的事务$T$收到一个操作，<code>read(T,x)</code>。但是$ts&lt;ts_{WR}(x)$。**换句话说，调度管理器发现一个对$x$的写操作在事务开始后已经完成（也即，在事务$T$启动到开始读之间，调度管理器发现一个对x的写操作在此期间完成了）。在这种情况下，事务$T$简单的被终止。**相反，如果$ts&gt;ts_{WR}(x)$，那么让读操作发生（即上一个对$x$的写操作是在事务$T$之前完成的）。此外，$ts_{RD}(x)$被设置为$max{ts, ts_{RD}(x)}$。</p><p>同样，假设调度管理器收到一个具有时间戳$ts$并包含写操作<code>write(T, x)</code>的事务$T$。**如果$ts&lt;ts_{RD}(x)$，那么它只能取消事务$T$，这是因为$x$的当前值已经被更晚的事务读过**（也即，在事务$T$启动到开始写之前完成了一个事物对$x$的读操作，实际上应该事务$T$先来的，但是被别人插队了，为了大局，事务$T$只能取消）。另一方面，如果$ts&gt;ts_{RD}(x)$，那么它改变$x$的值，因为没有更晚的事务读过它。$ts_{WR}$也被设置为$max{ts, ts_{WR}(x)}$。</p><p>时间戳具有与锁定不同的特性。当事务遇到一个更大（或更晚）的时间戳时，它中止，而在同样的情况下，如果使用锁定方法，事务将等待或立即执行（也即，要是被人插了队自己就中止退出）。另一方面，它不会造成死锁，这是一个很大的优点。</p><h2 id="乐观的时间戳排序"><a href="#乐观的时间戳排序" class="headerlink" title="乐观的时间戳排序"></a>乐观的时间戳排序</h2><p>第三种处理多个事务同时执行的方法是乐观的并发控制（optimistic concurrency）。此技术的思想是：不关心别人在干什么，继续做自己要做的事情。如果有问题，等到后面再考虑（许多政客也使用这个算法，注：个人认为西方政客会这样做，咱们中国的政客都是特别怕出事情担责任的）。实际上，相对来讲冲突是很少的，所以系统大部分时间都运行正常。</p><p>尽管冲突可能很少发生，但也不是不会发生，因此仍需要某种方法来处理冲突。乐观的并发控制所做的事情是跟踪哪些数据项被改写了。<strong>在某个事务被提交的时候，它检查其他所有事务，看看是否有某些数据项从这个事务开始后被改变了（即在某个事务提交的时候检查其数据项是否被其他事务改变了）。如果被改变了，该事务被终止；如果没有被改变，则该事务提交。</strong></p><p>乐观的并发控制算法最适合基于<strong>私有工作空间</strong>的实现。在这种方式下，每个事务私下改变自己的数据，不受别的事务的干涉。最终，新数据要么被提交、要么被释放，这是一个相对简单直接的方法。</p><p>乐观的并发控制的最大优点是它不会发生死锁，允许最大的并行性。这是因为没有进程需要等待一个锁。缺点是有时它可能会失败，那时事务将不得不再次执行。在负载较重的情况下，失败的可能性比较大，使得乐观的并发控制成为了一个较差的选择。</p><p>此外，对乐观的并发控制的研究主要集中在非分布式系统中。</p><p>注：本文内容摘自「分布式系统原理与范型」，供本人学习回顾使用。</p>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式系统原理与范型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>并发控制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式快照捕获算法</title>
    <link href="/2021/10/14/%E5%85%A8%E5%B1%80%E7%8A%B6%E6%80%81/"/>
    <url>/2021/10/14/%E5%85%A8%E5%B1%80%E7%8A%B6%E6%80%81/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在许多情况下，知道分布式系统所处的全局状态是很有用的。<strong>分布式系统的全局状态（Global State）包括每个进程的本地状态和当前正在传输中的消息，所谓正在传输中的消息即该消息已经被发送但还没有被交付。</strong></p><p>有很多原因表明，知道分布式系统的状态是很有用的。例如，当已知本地计算已经停止并且也没有消息在传输时，系统显然进入了一个不能继续前进的状态。此时，通过分析全局状态我们就能得知系统是进入了死锁还是分布式计算已经正确的结束了。</p><p>注：本文内容主要摘自「分布式系统原理与范型」，主要用于本人学习回顾。</p><h2 id="关于全局状态和分布式快照"><a href="#关于全局状态和分布式快照" class="headerlink" title="关于全局状态和分布式快照"></a>关于全局状态和分布式快照</h2><p>Chandy和Lamport在其1985年发表的论文<a href="https://lamport.azurewebsites.net/pubs/chandy.pdf">Distributed Snapshots：Determining Global States of Distributed Systems </a>中提出了一个简单直接的<strong>记录分布式系统全局状态</strong>的方法，该方法引入了<strong>分布式快照</strong>（Distribute Snapshot）的概念。</p><p><strong>分布式快照反映了该分布式系统可能处于的状态，且重要的是，该记录的状态是全局一致的。</strong>这意味着，如果快照已经记录了进程P收到了来自进程Q的一条信息，那么应该也记录了进程Q确实发送了这个消息。否则，快照将会记录一个已经被接受但从未被发送的消息。需要注意的是，相反的情况是可以接受的，也即快照可以记录一个已经被发送但还尚未被接受的消息。</p><p>全局状态的概念可以用一个被称为切口（cut）的示意图来表达。如下图所示，穿越进程P1、P2和P3时间轴的虚线表示的是一个一致的切口。该切口表示了「为每个进程记录的最后事件」，记录了事件$m_1$的发送和接受，以及事件$m_2$的发送。</p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/IMG_6746735F12FC-1.jpeg" alt="IMG_6746735F12FC-1" style="zoom: 50%;" /><p>作为对比，下图是一个不一致的切口，快照中记录了事件$m_1$的发送和接受，但是仅记录了事件$m_3$的接受而未记录其发送。</p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/IMG_CC2C77FC7968-1.jpeg" alt="IMG_CC2C77FC7968-1" style="zoom:50%;" /><h2 id="分布式快照捕获算法的描述"><a href="#分布式快照捕获算法的描述" class="headerlink" title="分布式快照捕获算法的描述"></a>分布式快照捕获算法的描述</h2><p>为了简化对分布式快照捕获算法的解释，我们假设分布式系统可用一个彼此通过<strong>单向点对点通信通道</strong>相连的进程集合来表示。例如，进程可能在任何进一步通信发生前首先建立TCP连接。</p><p>任何进程都可以启动该算法来捕获一个分布式快照。启动算法的<strong>进程P</strong>通过记录它自己的本地状态而启动。然后，它可以通过每个<strong>流出通道</strong>发送一个<strong>标记</strong>，表明接受者应该参与记录全局状态。</p><p>当<strong>进程Q</strong>通过一个<strong>进入通道C</strong>接收到一个<strong>标记</strong>，该进程根据它是否已经保存了本地状态来决定下一步动作。</p><ul><li>如果进程Q尚未保存其本地状态，它就先记录本地状态，然后也通过它自己的每个流出通道发送一个标记。</li><li>如果进程Q已经保存了其本地状态，则通道C上的标记表明Q应该记录该通道的状态。该状态是从进程Q上次记录了它自己的本地状态开始，到它接受了该标记为止，Q所接受到的消息序列组成。</li></ul><p>当一个进程接受并处理了它的所有进入通道的标记时，就认为该进程已经完成了算法中与它有关的部分。此时可以将它记录的本地状态和它为每个进入通道记录的状态收集起来，发送给发起此快照的进程P。后者随后分析当前状态。需要注意的是，与此同时，分布式系统可以作为一个整体继续正常运行，也即分布式快照算法不会影响分布式系统的正常运行。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/cut.drawio.png" alt="cut.drawio"></p><p>还有一个需要注意的地方是，因为任何进程都能发起该算法，所以可能同时存在几个快照。为此，标记上附有发起该快照的进程的标识符（可能还有一个版本号）。只有在进程已经通过它的每个进入通道接收到了某个标记后，它才能完成与该标记相关的快照的创建。</p>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式系统原理与范型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>全局状态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>逻辑时钟（Logical Clock）</title>
    <link href="/2021/10/13/%E9%80%BB%E8%BE%91%E6%97%B6%E9%92%9F/"/>
    <url>/2021/10/13/%E9%80%BB%E8%BE%91%E6%97%B6%E9%92%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文内容主要摘自《分布式系统原理与范型》，主要用于本人后续回顾学习用，建议阅读原书。</p><h2 id="关于时钟"><a href="#关于时钟" class="headerlink" title="关于时钟"></a>关于时钟</h2><p>几乎所有的计算机都有一个计时电路，我们一般会称这个计时电路为「时钟」（但它们并不是通常意义上的时钟，我们将其称为「计时器」可能会更恰当一些）。「时钟」与进程之间的协作和同步有密切的关系，多个进程之间是通过「事件」发生的「时间」来就「事件」的发生顺序达成一致的。在单机单时钟的情况下，如果这个时钟存在少许偏差是不会出现问题的，因为这台机器上的所有进程都使用同一个时钟，所以它们的内部仍然会保持一致。</p><h2 id="逻辑时钟"><a href="#逻辑时钟" class="headerlink" title="逻辑时钟"></a>逻辑时钟</h2><p>在许多应用中，只要所有的机器能维持一个全局统一的时间就够了，这个时间并不需要与真实时间一致。对于某类算法而言，重要的是时钟的内部一致性，而不是它们是否与真实时间接近。这类算法通常将时钟称为「逻辑时钟」（logical clock）。</p><p>Lamport在其著名的论文<a href="http://lamport.azurewebsites.net/pubs/time-clocks.pdf">「Time, Clocks, and the Ordering of Events in a Distributed System 」</a>中阐明了「尽管时钟同步是可能的，但它不是绝对必要」的观点。如果两个进程不进行交互，那么他们的时钟也无须同步，这是因为即使没有进行同步也察觉不出来，并且也不会产生问题。他指出，<strong>通常重要的不是所有的进程在时间上完全一致，而是它们在事件的发生顺序上要达成一致</strong>。</p><h2 id="Lamport时间戳"><a href="#Lamport时间戳" class="headerlink" title="Lamport时间戳"></a>Lamport时间戳</h2><p>为了同步逻辑时钟，Lamport定义了一个称为「先发生」（happens-before）的关系。表达式$a \rightarrow b$读作「a在b之前发生」，意思是所有进程一致认为事件a先发生，然后事件b才发生。这种「先发生」关系有两种情况：</p><ul><li>如果a和b是同一个进程中的两个事件，且a在b之前发生，则$a \rightarrow b$为真；</li><li>如果a是一个进程发送消息的事件，而b为另一个进程接受该消息的事件，则$a \rightarrow b$也为真。消息不可能在发送之前就被接受，也不可能在发送的同时被接受，这是因为消息需要一定时间才能到达接收端。</li></ul><p>「先发生」关系是一种传递关系，所以如果$a \rightarrow b$且$b \rightarrow c$，则有$a \rightarrow c$。如果事件x和y发生在两个互不交换消息的进程中（也不通过第三方间接交换消息），那么无论是$x \rightarrow y$还是$y \rightarrow x$都不为真。这两个事件被称为「并发的」（concurrent），这意味着无法说或者不必说这两个事件什么时候发生，哪个事件先发生。</p><p>我们需要一种测量事件的方法，使得对于每个事件a，我们都能为它分配一个所有进程都认可的时间值$C(a)$。同时这些时间值必须具有如下性质：</p><ul><li><p>如果$a \rightarrow b$，那么$C(a) &lt; C(b)$；</p></li><li><p>时钟时间值$C$必须总是前进（增加），不能倒退（减少），校正时间的方法是给时间加上一个正值而不是减去一个正值；</p></li></ul><p>如下图所示，三个进程运行在不同的机器上，每个机器以各自的速率工作。当进程A的时钟「滴答」了6次时，进程B的时钟「滴答」了8次，进程C的时钟「滴答」了10次。我们下面描述一个从进程A到B再到C的消息传递及响应过程：</p><ol><li><p>在0时刻，进程A将消息a发送给进程B，消息的传输时间取决于信任哪个时钟。不管怎样，当它到达进程B时，进程B的时钟为16。如果消息a上携带了其在进程A上打包的时钟值0，则进程B会推算其传输时间为$16 - 0 = 16$个时钟值。</p></li><li><p>进程B在其时钟值为24时，将消息b发送给进程C，在进程C的时钟值为40时到达进程C。</p></li><li><p>进程C在其时钟值为50时，将响应消息c发送给进程B，在进程B的时钟值为48时到达进程B。</p></li><li><p>进程B在其时钟值为56时，将响应消息d发送给进程A，在进程A的时钟值为54时到达进程A。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/logical_clock2.png" alt="logical_clock"></p></li></ol><p>分析以上过程，我们可以发现一个有意思的现象，响应消息从进程C时钟值为50时刻出发，却在进程B时钟值为48时刻到达进程B，响应消息d也有类似的现象——<strong>消息的到达时刻竟然比消息的发送时刻还要早</strong>，这显然是不合理的，必须避免这种情况发生。</p><p>Lamport给出的解决方案是直接遵循「先发生关系」。消息c在时钟值为50时离开，那么它只能在时钟值为51或更晚时到达。所以每个消息都应该携带发送者时钟的「发送时间」。<strong>当消息到达，并且接受者时钟显示的时间值比消息的发送时间早时，接受者就把它的时钟调到一个比发送时间大1的值</strong>。调整后的过程图如下所示。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/lamport_logical_clock.png" alt="lamport_logical_clock"></p><p>对这个算法稍作补充就可以满足全局时间的需要，即在每两个事件之间，时钟必须至少「滴答」一次。如果一个进程以相当快的速度连续发送或接收两个消息，那么它的时钟必须在这之间至少「滴答」一次。</p><p><strong>在某些情况下还需要一个附加条件，即两个事件不会精确的同时发生。为了达到这个目标，我们可以将事件发生的进程号附加在时间的低位后，并用小数点分隔开。</strong>使用这种方法，如果进程1和进程2同时在40时刻发生了一个事件，那么前者可以标记为「40.1」，后者可以标记为「40.2」。</p><p>通过使用这种方法，我们现在有了一个为分布式系统中的所有事件分配时间的方法，这遵循下面的规则：</p><ol><li>若在同一进程中a在b之前发生，则$C(a)&lt;C(b)$；</li><li>若a和b分别代表发送一个消息和接受该消息的事件，则C(a)&lt;C(b)；</li><li>对于所有不同的事件a和b，$C(a) \neq C(b)$；</li></ol><p>这个算法为我们提供了一种对系统中所有的事件进行完全排序的方法。许多其他的分布式算法都需要这种排序以避免混淆，所以此算法在各种文献被广泛引用。</p><h2 id="向量时间戳"><a href="#向量时间戳" class="headerlink" title="向量时间戳"></a>向量时间戳</h2><p>Lamport时间戳导致分布式系统中的所有事件都要经过排序以具有这样的性质：如果事件a发生在事件b之前，那么a也应该排在b之前，即$C(a) &lt; C(b)$。</p><p>然而，使用Lamport时间戳后，只通过比较事件a和b各自的时间值$C(a)$和$C(b)$，无法说明它们之间的关系。换句话说，$C(a) &lt; C(b)$不能说明事件a就是在事件b之前发生的。问题在于Lamport时间戳不能捕获因果关系（causality）。</p><p>因果关系可以通过向量时间戳（Vector Timestamp）来捕获。分配给<strong>事件a</strong>的**向量时间戳 $VT(a)$**具有下列性质：如果对于某一事件b，有 $VT(a) &lt; VT(b)$，那么认为事件a在因果关系上处于事件b之前。</p><p>向量时间戳的创建是通过让**每个进程$P_n$维护一个向量$V_n$**来实现的，该向量具有下面两个性质：</p><ul><li>$V_i[i]$是到目前为止<strong>进程</strong>$P_i$发生的事件的数量；</li><li>如果$V_i[j] = k$，那么<strong>进程</strong>$P_i$知道<strong>进程</strong>$P_j$中已经发生了$k$个事件；</li></ul><p>第一个性质是通过在**进程$P_i$中的新事件发生时递增$V_i[i]$**来维护的。</p><p>第二个性质是通过在所发送的消息中携带向量时间戳来维护的，<strong>当进程$P_i$发送消息$m$时，它将自己的当前向量作为时间戳$vt(m)$一起发送。</strong></p><p>通过使用这种方式，接受者可以得知进程$P_i$中已经发生的事件数。更重要的是，接受者可以得知在进程$P_i$发送消息$m$之前其他进程已经发生了多少个事件。<strong>换句话说，消息$m$的时间戳$vt(m)$告诉接受者其他进程中有多少事件发生在消息$m$之前，并且$m$可能在因果关系上依赖于这些事件。</strong></p><p>进程$P_j$依赖于其接收到的消息来调整自己所维护的向量。<strong>当进程$P_j$在接收到消息$m$时，它调整自己的向量，将每项$V_j[k]$设置为$max{V_j[k], vt(m)[k]}$。该向量现在反映了进程$P_j$所必须接受到消息数目，该消息数目至少是在发送$m$之前见到的消息。此后将$V_j[i]$增1，这表示接受消息$m$的事件是来自于进程$P_i$的下一个事件。</strong></p><p>只在不违背因果关系限制时，才能使用向量时间戳来传递消息。</p><p><strong>示例：</strong></p><p>我们来考虑一个电子公告板的例子。当进程$P_i$张贴一篇文章时，它将改文章作为消息$a$广播出去，并且在该消息上附加一个时间戳$vt(a)$，其值等于$V_i$。当另一个进程$P_j$接收到消息$a$时，它将根据其携带的时间戳$vt(a)$来调整自己的向量，以使$V_j[i] &gt; vt(a)[i]$。</p><p>假设进程$P_j$在收到消息$a$后广播了一个该文章的回复消息$r$，消息$r$携带值等于$V_j$的时间戳$vt(r)$。需要注意的是$vt(r)[i] &gt; vt(a)[i]$。假设通信是可靠的，包含文章的消息$a$和包含回复的消息$r$最终都到达了另一个进程$P_k$。</p><p>因为我们没有对消息的顺序关系做出假设，所以消息$r$可能在消息$a$之前到达进程$P_k$。进程$P_k$接受到消息$r$时检查其时间戳，并决定推迟提交消息$r$，直到因果关系上位于$r$之前的消息都接受到了才提交$r$。消息$r$ 只有在满足下列条件时才得到交付：</p><ol><li>$vt(r)[j] = V_k[j] + 1$；</li><li>对于所有满足$i \neq j$的$i$和$j$，$vt(r)[i] &lt; V_k[i]$；</li></ol><p>第一个条件说明$r$是进程$P_k$正在等待的下一条来自进程$P_j$的消息；</p><p>第二个条件说明当进程$P_j$发送消息$r$时，进程$P_k$只看到被进程$P_j$看到的消息。这意味着进程$P_k$已经看到了消息$a$。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/because.drawio.png" alt="because.drawio"></p>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>分布式系统原理与范型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>时钟同步</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Google File System</title>
    <link href="/2021/10/05/The%20Google%20File%20System/"/>
    <url>/2021/10/05/The%20Google%20File%20System/</url>
    
    <content type="html"><![CDATA[<h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><h2 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h2><h3 id="论文名"><a href="#论文名" class="headerlink" title="论文名"></a>论文名</h3><p>The Google File System</p><h3 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h3><p>Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung</p><h3 id="期刊-会议"><a href="#期刊-会议" class="headerlink" title="期刊/会议"></a>期刊/会议</h3><p> SOSP’03, October 19–22, 2003, Bolton Landing, New York, USA. </p><h2 id="论文摘要"><a href="#论文摘要" class="headerlink" title="论文摘要"></a>论文摘要</h2><h2 id="论文地址"><a href="#论文地址" class="headerlink" title="论文地址"></a>论文地址</h2><h3 id="原文地址"><a href="#原文地址" class="headerlink" title="原文地址"></a>原文地址</h3><p><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf">「The Google File System」</a></p><h3 id="阅读参考"><a href="#阅读参考" class="headerlink" title="阅读参考"></a>阅读参考</h3><p><a href="http://duanple.com/?p=202">「google论文二Google文件系统(上)」</a></p><h1 id="阅读摘要-amp-笔记"><a href="#阅读摘要-amp-笔记" class="headerlink" title="阅读摘要&amp;笔记"></a>阅读摘要&amp;笔记</h1><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients.</p><p>我们设计实现了Google文件系统，一个应用于大型分布式数据密集型应用程序的可扩展分布式文件系统。它在运行于廉价硬件设备的同时提供容错性，并为大量的客户端提供高聚合性能。</p><p>While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points.</p><p>虽然于很多之前的分布式文件系统有相同的目标，但是我们的设计是基于对我们的应用程序负载和技术环境的观察所驱动的，这反映了与早先的分布式文件系统的设计思想的明显背离。这促使我们重新审视传统的选择，并探索根本不同的设计要点。</p><p>The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients.</p><p>此文件系统成功的满足了我们的存储需要。它被广泛的部署在Google内部，并且作为产生和处理我们的服务所需要的数据，以及需要大型数据集的研究和开发工作的存储平台。迄今为止最大的集群在超过一千台机器上的数千个磁盘上提供数百 TB 的存储，并且它被数百个客户端同时访问。</p><p>In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.</p><p>在本文中，我们介绍了旨在支持分布式应用程序的文件系统接口扩展，讨论了我们设计的许多方面，并报告了来自微基准测试和现实世界使用的测试结果。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>GFS shares many of the same goals as previous distributed file systems such as performance, scalability, reliability, and availability.</p><p>GFS和之前的大多数分布式系统一样，其主要设计目标是：性能、可扩展性、可靠性和可用性。</p><p>However, its design has been driven by key observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system design assumptions.</p><p>与早先的分布式系统的区别在于，它的设计是由我们当前和预期的应用负载和技术环境所驱动的。</p><p>We have reexamined traditional choices and explored radically different points in the design space.</p><p>我们重新审视了传统设计的选择，并在设计空间上探索了根本不同的设计要点。</p><ul><li>First, component failures are the norm rather than the exception.Therefore, constant monitoring, error detection, fault tolerance, and automatic recovery must be integral to the system.</li><li>Second, files are huge by traditional standards. Multi-GB files are common.As a result, design assumptions and parameters such as I/O operation and block sizes have to be revisited.</li><li>Third, most files are mutated by appending new data rather than overwriting existing data.Random writes within a file are practically non-existent. Once written, the files are only read, and often only sequentially.Given this access pattern on huge files, appending becomes the focus of performance optimization and atomicity guarantees, while caching data blocks in the client loses its appeal.</li><li>Fourth, co-designing the applications and the file system API benefits the overall system by increasing our flexibility.For example, we have relaxed GFS’s consistency model to vastly simplify the file system without imposing an onerous burden on the applications. We have also introduced an atomic append operation so that multiple clients can append concurrently to a file without extra synchronization between them.</li><li>我们将设备故障视为常态而不是意外。因此，持续监控、错误检测、容错性和自动恢复性必须作为系统不可或缺的一部分。</li><li>按照传统的标准，文件是巨大的。GB大小的文件是常态。因此必须重新考虑设计假设和参数，例如I/O操作和块大小。</li><li>大多数文件是通过追加操作而不是覆盖写来改变的。随机写操作很少出现，一旦写入后，大多数文件通常是只读的，并且是顺序读取。鉴于这种大文件的访问模式，追加操作成为性能优化和原子性保证的重点，而客户端的数据块缓存则失去了吸引力。</li><li>结合应用程序和文件系统一块设计API，能通过提高灵活性来使整个系统收益。例如，我们通过放松对GFS的一致性模型来简化文件系统，而不会对应用程序带来较大的负担。我们还通过引入原子性的追加操作来使多个客户端可以并发行的对同一个文件进行追加操作，而无需在它们之间进行额外的同步。</li></ul><h1 id="2-Design-Overview"><a href="#2-Design-Overview" class="headerlink" title="2 Design Overview"></a>2 Design Overview</h1><h2 id="2-1-Assumptions"><a href="#2-1-Assumptions" class="headerlink" title="2.1 Assumptions"></a>2.1 Assumptions</h2><p>We alluded to some key observations earlier and now lay out our assumptions in more details.</p><p>我们之前提到了一些关键性的观察结果，现在我们对它们进行更详细的描述。</p><ul><li>The system is built from many inexpensive commodity components that often fail. It must constantly monitor itself and detect, tolerate, and recover promptly from component failures on a routine basis.</li><li>The system stores a modest number of large files.Multi-GB files are the common case<br>and should be managed efficiently. Small files must be supported, but we need not optimize for them.</li><li>The workloads primarily consist of two kinds of reads: large streaming reads and small random reads.<ul><li>In large streaming reads, individual operations typically read hundreds of KBs, more commonly 1 MB or more.Successive operations from the same client often read through a contiguous region of a file.</li><li>A small random read typically reads a few KBs at some arbitrary offset. Performance-conscious applications often batch and sort their small reads to advance steadily through<br>the file rather than go back and forth.</li></ul></li><li>The workloads also have many large, sequential writes that append data to files. Typical operation sizes are similar to those for reads. Once written, files are seldom modified again. Small writes at arbitrary positions in a file are supported but do not have to be efficient.</li><li>The system must efficiently implement well-defined semantics for multiple clients that concurrently append to the same file.</li><li>High sustained bandwidth is more important than low latency. Most of our target applications place a premium on processing data in bulk at a high rate, while few have stringent response time requirements for an individual read or write.</li><li>系统由很多经常出问题的廉价设备组成。它们必须持续自我监视，并定期检测、容错并迅速从组件故障中恢复。</li><li>系统中存储着大量的大文件数据。GB大小的文件是作为常态存在的，并且必须进行高效的管理。同时，系统也必须支持小型文件，但不必对此进行优化。</li><li>工作负载主要由两种读操作组成：大规模的流读取和小规模的随机读取。<ul><li>在大规模的流读取中，每次操作通常读取几百KB，更常见的是1MB或更多。来自同一客户端连续读操作通常会读取文件的连续区域。</li><li>一个小的随机读操作通常会以任意偏移量读取几KB。注重性能的应用程序通常会对这些小的读取操作进行批处理和排序，以持续稳定推进文件的读操作，而不是来回读取。</li></ul></li><li>工作负载中也会有很多大的连续写操作追加数据到文件。一般数据大小和读操作类似。一旦写入后文件很少被修改。小的任意写操作必须支持，但不必支持高效性。</li><li>系统必须实现高效的、良好定义的语义，以支持大量客户端对同一文件的并发追加写操作。</li><li>高持续带宽比低延迟更重要。我们的大多数目标应用程序都非常重视以高速率批量处理数据，而对单个读或写操作的响应时间并没有严格的要求。</li></ul><h2 id="2-2-Interface"><a href="#2-2-Interface" class="headerlink" title="2.2 Interface"></a>2.2 Interface</h2><p>GFS provides a familiar file system interface, though it does not implement a standard API such as POSIX. Files are organized hierarchically in directories and identified by pathnames. We support the usual operations to <code>create</code>, <code>delete</code>, <code>open</code>, <code>close</code>, <code>read</code>, and <code>write</code> files.</p><p>GFS提供了一个熟悉的文件系统接口，虽然它并没有实现像POSIX标准那样的标准API接口。文件在目录中分层组织，并由路径名标识。我们还支持文件的一些常见操作，如<code>create</code>、<code>delete</code>、<code>open</code>、<code>close</code>、<code>read</code>、<code>write</code>。</p><p>Moreover, GFS has <code>snapshot</code> and <code>record append</code> operations. Snapshot creates a copy of a file or a directory tree at low cost. Record append allows multiple clients to append data to the same file concurrently while guaranteeing the atomicity of each individual client’s append. It is useful for implementing multi-way merge results and producer-consumer queues that many clients can simultaneously append to without additional locking. We have found these types of files to be invaluable in building large distributed applications. Snapshot and record append are discussed further in Sections 3.4 and 3.3 respectively.</p><p>此外，GFS支持<strong>快照</strong>和<strong>追加</strong>写操作。快照会以低开销创建一个文件或目录树的拷贝。追加写操作允许大量客户端并发向同一个文件追加写，并且保证每个客户端的追加写都是原子性的。这对实现多路合并操作和生产者-消费者队列非常有用，许多客户端可以同时进行追加操作而不需要额外的加锁处理。我们发现这些类型的文件对构建大型分布式应用非常宝贵。快照和追加写操作将在3.4节和3.3节中进一步讨论。</p><h2 id="2-3-Architecture"><a href="#2-3-Architecture" class="headerlink" title="2.3 Architecture"></a>2.3 Architecture</h2><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/GFS_Figure1.png" alt="Untitled"></p><p>A GFS cluster consists of a single master and multiple chunk-servers and is accessed by multiple clients, as shown in Figure 1.</p><p>如图1所示，一个GFS集群由一个Master和大量的被很多Client访问的Chunk Server组成。</p><p>Files are divided into fixed-size chunks. Each chunks identified by an immutable and globally unique 64 bit chunk handle assigned by the master at the time of chunk creation.Chunk servers store chunks on local disks as Linux files and read or write chunk data specified by a chunk handle and byte range. For reliability, each chunks replicated on multiple chunk servers. By default, we store three replicas, though users can designate different replication levels for different regions of the file namespace.</p><p>文件被划分为固定大小的Chunk。每个Chunk在创建时由Master分配一个不可变的，并且全局唯一的64位Chunk Handle标识。Chunk Server将Chunks作为Linux文件存储在本地磁盘上，读写操作都由Chunk Handle和字节边界来明确。为了可靠性，每个Chunk被复制存储到多个Chunk Server上。虽然用户可以为文件命名空间的不同区域指定不同的备份级别，但通常默认为三个备份。</p><p>The master maintains all file system metadata. This includes the namespace, access control information, the mapping from files to chunks, and the current locations of chunks. It also controls system-wide activities such as chunk lease management, garbage collection of orphaned chunks, and chunk migration between chunk servers. The master periodically communicates with each chunk server in HeartBeat messages to give it instructions and collect its state.</p><p>集群Master保存了文件系统的所有元数据。这包括，命名空间、访问控制信息、文件和Chunk的映射关系，以及Chunk的当前保存位置。它也控制系统范围内的一些活动，比如Chunk租约管理、孤立块的垃圾回收，以及Chunk在Chunk Server之间的迁移。 Master还会周期性的和Chunk Server进行交流，通过心跳信息下发指令和收集Chunk Server状态。</p><p>GFS client code linked into each application implements the file system API and communicates with the master and chunk servers to read or write data on behalf of the application. Clients interact with the master for metadata operations, but all data-bearing communication goes directly to the chunk servers.</p><p>链接到应用程序的客户端代码实现了文件系统API，并且代表应用程序与Master和Chunk Server通信以读写数据。客户端与主机交互以进行元数据操作，但所有的数据通信都是直接发送到Chunk Server。</p><p>Neither the client nor the chunk server caches file data. Client caches offer little benefit because most applications stream through huge files or have working sets too large to be cached. Not having them simplifies the client and the overall system by eliminating cache coherence issues. (Clients do cache metadata, however.) Chunk servers need not cache file data because chunks are stored as local files and so Linux’s buffer cache already keeps frequently accessed data in memory.</p><p>客户端和Chunk Server都不需要缓存文件数据。客户端缓存并不会带来明显的好处，因为大多数应用程序通过数据流与大型文件传输数据，或者因为数据量太大而无法缓存。去掉缓存可以简化客户端，以及整个系统（没有因缓存一致性而带来的问题）。Chunk Server不需要缓存文件数据，因为Chunk是作为本地文件存储的，所以Linux的Cache缓存已经将经常访问的数据保存在了内存中。（利用了操作系统的缓存）</p><h2 id="2-4-Single-Master"><a href="#2-4-Single-Master" class="headerlink" title="2.4 Single Master"></a>2.4 Single Master</h2><p>Having a single master vastly simplifies our design and enables the master to make sophisticated chunk placement and replication decisions using global knowledge.However, we must minimize its involvement in reads and writes so that it does not become a bottleneck. Clients never read and write file data through the master. Instead, a client asks the master which chunk servers it should contact. It caches this information for a limited time and interacts with the chunk servers directly for many subsequent operations.</p><p>仅有一个Master大大简化了我们的设计，并且使其能够利用全局知识做出复杂的决策以确定Chunk的放置位置和复制。但是，我们必须减少其所参与的读写操作，以保证它不会成为整个系统的瓶颈。客户端只是通过向Master询问它应该联系的Chunk Server信息，而不是通过Master直接读写数据，并且客户端会将其请求得到的信息缓存一段时间，在此时间段内它可以与Chunk Server直接交互而不需要向Master询问信息。</p><p>Let us explain the interactions for a simple read with reference to Figure 1.</p><ul><li>First, using the fixed chunk size, the client translates the file name and byte offset specified by the application into a chunk index within the file.</li><li>Then, it sends the master a request containing the file name and chunk index.</li><li>The master replies with the corresponding chunk handle and locations of the replicas. The client caches this information using the file name and chunk index as the key.</li><li>The client then sends a request to one of the replicas, most likely the closest one. The request specifies the chunk handle and a byte range within that chunk. Further reads of the same chunk require no more client-master interaction until the cached information expires or the file is reopened.</li></ul><p>In fact, the client typically asks for multiple chunks in the same request and the master can also include the information for chunks immediately following those requested. This extra information sidesteps several future client-master interactions at practically no extra cost.</p><p>我们根据图1来解释一个读操作的交互过程：</p><ul><li>首先，根据Chunk的固定大小，客户端通过应用程序中标识的文件名和字节偏移量转换为Chunk索引。</li><li>然后，客户端将包含文件名和Chunk索引的请求信息发送给Master。</li><li>Master向客户端响应Chunk Handle和一个Chunk副本的位置信息，客户端使用Chunk索引作为键值对中的键来缓存该信息。</li><li>客户端随后会向一个Chunk副本发送请求信息，通常是距离较近的副本。请求会标识Chunk Handle和字节边界。直到缓存信息到期或重新打开该文件前，客户端请求同一Chunk都不再需要和Master交互。</li></ul><p>实际上，会在一个查询请求信息中包含多个Chunk请求，Master也会将多个Chunk的信息封装在一个响应包中发回给客户端。这些额外的信息不需要其他成本就可以减少客户端和Master的几次交互操作。</p><h2 id="2-5-Chunk-Size"><a href="#2-5-Chunk-Size" class="headerlink" title="2.5 Chunk Size"></a>2.5 Chunk Size</h2><p>Chunk size is one of the key design parameters. We have chosen 64 MB, which is much larger than typical file system block sizes. Each chunk replica is stored as a plain Linux file on a chunk server and is extended only as needed. Lazy space allocation avoids wasting space due to internal fragmentation, perhaps the greatest objection against such a large chunk size.</p><p>Chunk的大小是设计中的关键参数。我们选择的64MB大小，远远大于典型的文件系统的块大小。每个Chunk副本都作为普通的Linux文件存储在Chunk Server中，并且只有在需要时才会进行扩展。懒空间分配避免了由于内部碎片所导致的空间浪费，可能产生的最大的碎片有一个Chunk那么大。</p><blockquote><p>惰性空间分配：使用惰性空间分配时，空间的物理分配会尽可能延迟，直到累积了块大小大小的数据(在GFS的默认情况为下为64 MB)。</p></blockquote><p>A large chunk size offers several important advantages.</p><ul><li>First, it reduces clients’ need to interact with the master because reads and writes on the same chunk require only one initial request to the master for chunk location information.</li><li>Second, since on a large chunk, a client is more likely to perform many operations on a given chunk, it can reduce network overhead by keeping a persistent TCP connection to the chunk server over an extended period of time.</li><li>Third, it reduces the size of the metadata stored on the master. This allows us to keep the metadata in memory, which in turn brings other advantages that we will discuss in Section 2.6.1.</li></ul><p>大的Chunk大小带来了以下重要的优势：</p><ul><li>首先，这减少了Client与Master多次交互的需要，因为Client可以将Chunk的位置信息缓存到本地，所以对于同一个Chunk的读写操作，Client只需要与Master进行一次请求。</li><li>其次，对于一个较大的Chunk，客户端可能在此块上进行更多的操作，这样就可以通过延长TCP连接时间来减少网络开销。</li><li>最后，这减少了存储在Master上的元数据大小。这就可以使master在内存保存更多的元数据，反过来这就带来了我们在2.6.1节中讨论的其他优势。</li></ul><p>On the other hand, a large chunk size, even with lazy space allocation, has its disadvantages. A small file consists of a small number of chunks, perhaps just one. The chunk servers storing those chunks may become hot spots if many clients are accessing the same file. In practice, hot spots have not been a major issue because our applications mostly read large multi-chunk files sequentially.</p><p>另一方面，使用大的Chunk，即使利用了惰性空间分配，也存在它的缺点。一个小型文件只有几个Chunk组成，甚至可能只有一个。存储这些Chunk的Chunk Server可能因为会被大量客户端访问同一个文件而成为访问热点。实际上，访问热点并不是一个主要的问题，因为我们的应用主要是顺序读取那些由很多Chunk组成的大文件。</p><h2 id="2-6-Metadata"><a href="#2-6-Metadata" class="headerlink" title="2.6 Metadata"></a>2.6 Metadata</h2><p>The master stores three major types of metadata: the file and chunk namespaces, the mapping from files to chunks, and the locations of each chunk’s replicas. </p><p>Master存储三种类型的元数据：文件和Chunk命名空间、文件到Chunk的映射关系、每个Chunk副本的位置信息。</p><p>All metadata is kept in the master’s memory. The first two types (namespaces and file-to-chunk mapping) are also kept persistent by logging mutations to an operation log stored on the master’s local disk and replicated on remote machines. Using a log allows us to update the master state simply, reliably, and without risking inconsistencies in the event of a master crash. The master does not store chunk location information persistently. Instead, it asks each chunk server about its chunks at master startup and whenever a chunk server joins the cluster.</p><p>所有的元数据信息都被Master保存在它的内存中。前两种类型的元数据（命名空间和文件到Chunk的映射）还通过操作日志更新到本地磁盘以及备份到远程的机器来保证数据持久化。使用日志可以使我们简单、可靠的更新Master的状态，而不用担心在Master故障时造成不一致性的危险。Master不会持久化的存储Chunk的位置信息，因为，它会在启动的时候，以及Chunk Server加入集群的时候询问每个Chunk Server的Chunk信息。</p><h3 id="2-6-1-In-Memory-Data-Structures"><a href="#2-6-1-In-Memory-Data-Structures" class="headerlink" title="2.6.1 In-Memory Data Structures"></a>2.6.1 In-Memory Data Structures</h3><p>Since metadata is stored in memory, master operations are fast. Furthermore, it is easy and efficient for the master to periodically scan through its entire state in the background. This periodic scanning is used to implement chunk garbage collection, re-replication in the presence of chunk server failures, and chunk migration to balance load and disk space usage across chunk servers. Sections 4.3 and 4.4 will discuss these activities further.</p><p>元数据被存储到Master的内存中后，Master对元数据的操作会非常快。此外，Master能在后台简单有效的完成对整体状态的周期性扫描。这个周期性的扫描主要用于：Chunk的垃圾回收，Chunk Server出现问题时的重新复制，Chunk迁移以平衡负载，以及Chunk Server的磁盘空间利用。4.3节和4.4节将会进一步讨论这些活动。</p><p>One potential concern for this memory-only approach is that the number of chunks and hence the capacity of the whole system is limited by how much memory the master has. This is not a serious limitation in practice. The master maintains less than 64 bytes of metadata for each 64 MB chunk. Most chunks are full because most files contain many chunks, only the last of which may be partially filled. Similarly, the file namespace data typically requires less than 64 bytes per file because it stores file names compactly using prefix compression.</p><p>这种使用内存的方法潜存的一种担心是，这些Chunk的数量以及整个系统的容量会受到Master内存空间的限制。实际上，这并不是一个严重的问题。Master对于每个64MB大小的Chunk保存少于64字节的元数据。因为大多数文件包含大量的Chunk，因此大多数Chunk都是满的，可能只有最后一个Chunk是部分填充后的。类似的，文件命名空间通常对于每个文件也只需要少于64字节的，因为它们存储的文件名是使用前缀紧凑压缩后的。</p><p>If necessary to support even larger file systems, the cost of adding extra memory to the master is a small price to pay for the simplicity, reliability, performance, and flexibility we gain by storing the metadata in memory.</p><p>即使需要支持更大的文件系统，为Master增加内存来将元数据存储在内存中，以获得简单型、可靠性、性能以及灵活性而花费的额外的钱也是很小的代价。</p><h3 id="2-6-2-Chunk-Locations"><a href="#2-6-2-Chunk-Locations" class="headerlink" title="2.6.2 Chunk Locations"></a>2.6.2 Chunk Locations</h3><p>The master does not keep a persistent record of which chunk servers have a replica of a given chunk. It simply polls chunk servers for that information at startup. The master can keep itself up-to-date thereafter because it controls all chunk placement and monitors chunk server status with regular HeartBeat messages.<br>Master并不会持久化那些拥有Chunk副本的Chunk Server给定的每个Chunk记录。它只是会在启动时轮询每个Chunk Server来获得这些信息。Master可以在此之后保证自己持有最新的信息，因为它控制Chunk的放置位置，以及通过与Chunk Server交换心跳信息来监听Chunk Server 状态。</p><p>We initially attempted to keep chunk location information persistently at the master, but we decided that it was much simpler to request the data from chunk servers at startup, and periodically thereafter. This eliminated the problem of keeping the master and chunk servers in sync as chunk servers join and leave the cluster, change names, fail, restart, and so on. In a cluster with hundreds of servers, these events happen all too often.</p><p>我们一开始也尝试过在Master中持久化Chunk的位置信息，但是后来我们发现，在启动时向每个Chunk Server来请求数据是更简单的方法。这消除了在Chunk Server加入和离开集群、更改名称、失败、重启等时，Master必须与Chunk Server保持同步的问题。而这些问题在具有几百台服务器的集群中是经常发生的。</p><p>Another way to understand this design decision is to realize that a chunk server has the final word over what chunks it does or does not have on its own disks. There is no point in trying to maintain a consistent view of this information on the master because errors on a chunk server may cause chunks to vanish spontaneously (e.g., a disk may go bad and be disabled) or an operator may rename a chunk server.</p><p>理解此设计决策的另一种方式是，意识到Chunk Server对它自己有没有某个Chunk拥有最终发言权（即只有Chunk Server才能确定他自己到底有没有某个Chunk）。试图在Master上维护此信息的一致性视图是没有意义的，因为在Chunk Server上发生的各种错误都可能会导致存储的Chunk自发性的消失（例如，磁盘可能会发生故障而无法使用），或者一个操作员可能会重命名一个Chunk Server。</p><h3 id="2-6-3-Operation-log"><a href="#2-6-3-Operation-log" class="headerlink" title="2.6.3 Operation log"></a>2.6.3 Operation log</h3><p>The operation log contains a historical record of critical metadata changes. It is central to GFS. Not only is it the only persistent record of metadata, but it also serves as a logical time line that defines the order of concurrent operations. Files and chunks, as well as their versions (see Section 4.5), are all uniquely and eternally identified by the logical times at which they were created.</p><p>操作日志包含了关键元数据改变的历史记录。它是GFS的核心。它不仅是元数据的唯一持久化数据，并且还充当定义并发操作顺序的时间线。文件和Chunk，以及它们的版本，都由它们创建时的逻辑时间唯一且永久标识。</p><p>Since the operation log is critical, we must store it reliably and not make changes visible to clients until metadata changes are made persistent. Otherwise, we effectively lose the whole file system or recent client operations even if the chunks themselves survive. Therefore, we replicate it on multiple remote machines and respond to a client operation only after flushing the corresponding log record to disk both locally and remotely. The master batches several log records together before flushing thereby reducing the impact of flushing and replication on overall system throughput.</p><p>因为操作日志非常重要，因此我们必须可靠的存储它，并且在对元数据的更改持久化之前不能使这些改变对客户端可见。否则，我们很可能失去整个的文件系统或最近的客户端操作，即使Chunks保存下来。因此，我们将其复制到多台远程的机器上，并且仅在本地和远程的机器将相应的日志刷新到磁盘上之后才向客户端响应。Master在刷新前将操作记录进行批处理，以减少刷新和复制对整个系统吞吐量的影响。</p><p>The master recovers its file system state by replaying the operation log. To minimize startup time, we must keep the log small. The master checkpoints its state whenever the log grows beyond a certain size so that it can recover by loading the latest checkpoint from local disk and replaying only the limited number of log records after that. The checkpoint is in a compact B-tree like form that can be directly mapped into memory and used for namespace lookup without extra parsing. This further speeds up recovery and improves availability.</p><p>Master通过重放操作日志来恢复其文件系统状态。为了减少启动时间，我们必须使日志记录尽可能的小。每当日志记录超过指定的大小时，Master都会检查它自身的状态，以便它可以通过从本地磁盘加载最新的检查点，再通过重放有限数量的日志记录来恢复文件系统状态。检查点采用类似B树的紧凑形式，可以直接映射到内存中，在无需额外解析的下用于命名空间的查找。这加速了文件系统的恢复过程，并且提高了可用性。</p><p>Because building a checkpoint can take a while, the master’s internal state is structured in such a way that a new checkpoint can be created without delaying incoming mutations. The master switches to a new log file and creates the new checkpoint in a separate thread. The new checkpoint includes all mutations before the switch. It can be created in a minute or so for a cluster with a few million files. When completed, it is written to disk both locally and remotely.</p><p>由于构建检查点需要一定的时间，因此，Master的内部状态以这样的一种方式来构建检查点，即在不延迟传入新的突变的情况下构建新的检查点。Master切换到一个新的日志文件中，并且在一个单独的线程中创建一个新的检查点。新的检查点包含在切换之前的所有突变。它需要1分钟左右的时间来为具有几百个文件的集群来创建检查点。创建完成后，它会被写到本地和远程的磁盘中。</p><p>Recovery needs only the latest complete checkpoint and subsequent log files. Older checkpoints and log files can be freely deleted, though we keep a few around to guard against catastrophes. A failure during checkpointing does not affect correctness because the recovery code detects and skips incomplete checkpoints.</p><p>文件系统恢复只需要最新的完整检查点以及后续的日志文件。较旧的检查点和日志文件可以自由删除，但我们也保留了一些来为抵抗灾难做保证。检查点期间的错误不会影响正确性，因为恢复代码会检测并跳过不完整的检查点。</p><h2 id="2-7-Consistency-Model"><a href="#2-7-Consistency-Model" class="headerlink" title="2.7 Consistency Model"></a>2.7 Consistency Model</h2><p>GFS has a relaxed consistency model that supports our highly distributed applications well but remains relatively simple and efficient to implement. We now discuss GFS’s guarantees and what they mean to applications. We also highlight how GFS maintains these guarantees but leave the details to other parts of the paper.</p><p>GFS使用的相对宽松的一致性模型不但能很好的支持我们的高度分布式应用程序，而且保证了实现上的简单和高效。我们现在讨论GFS所提供的保证，以及它们对应用程序来说意味着什么。我们还会强调GFS如何维护这些保证，但会将具体的细节留给论文的其他部分来描述。</p><h3 id="2-7-1-Guarantees-by-GFS"><a href="#2-7-1-Guarantees-by-GFS" class="headerlink" title="2.7.1 Guarantees by GFS"></a>2.7.1 Guarantees by GFS</h3><p>File namespace mutations (e.g., file creation) are atomic.They are handled exclusively by the master: namespace locking guarantees atomicity and correctness (Section 4.1); the master’s operation log defines a global total order of these operations (Section 2.6.3).</p><p>文件命名空间的改变是原子性的。它们仅由Master来处理：命名空间锁来保证原子性和正确性；Master的操作日志定义了这些操作的全局总顺序。</p><p><img src="https://ap0l1o.oss-cn-qingdao.aliyuncs.com/img/GFS_Table1.png" alt="Untitled"></p><p>The state of a file region after a data mutation depends on the type of mutation, whether it succeeds or fails, and whether there are concurrent mutations. Table 1 summarizes the result. </p><p>数据变更后文件区域的状态依赖于变更的类型，数据变更成功还是失败，以及是否存在并发行变更。表1对这些结果进行了总结。</p><p>A file region is consistent if all clients will always see the same data, regardless of which replicas they read from. A region is defined after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety. </p><ul><li>When a mutation succeeds without interference from concurrent writers, the affected region is defined (and by implication consistent): all clients will always see what the mutation has written.</li><li>Concurrent successful mutations leave the region undefined but consistent: all clients see the same data, but it may not reflect what any one mutation has written. Typically, it consists of mingled fragments from multiple mutations.</li><li>A failed mutation makes the region inconsistent (hence also undefined): different clients may see different data at different times.</li></ul><p>如果客户端无论从哪个副本都能看到一样的数据，则文件区域是一致性的。如果一个文件数据在变更后是一致性的，并且客户端能看到变更写入的内容，则该区域是定义良好的。</p><ul><li>当一个变更成功并且不受其他并发写入的干扰时，受影响的区域是定义良好的（同时意味着是一致性的）：所有客户端都能看到变更所写入的内容。</li><li>并发的成功变更所影响的区域是一致的，但不是定义良好的：所有的客户端都能看到相同的数据，但并不能反映出每个变更所写入的内容。通常，这由多个变更混合片段组成。</li><li>一个失败的变更会导致区域处于不一致的状态（因此也不是定义良好的）：不同的客户端在不同的时间段能看到不同的数据。</li></ul><p>We describe below how our applications can distinguish defined regions from undefined regions. The applications do not need to further distinguish between different kinds of undefined regions.</p><p>我们会在下面描述我们的应用程序如何区分定义良好的区域和非定义良好的区域。应用程序不需要进一步区分各种不同的非定义良好的区域。</p><p>Data mutations may be writes or record appends. A write causes data to be written at an application-specified file offset. A record append causes data (the “record”) to be appended atomically at least once even in the presence of concurrent mutations, but at an offset of GFS’s choosing (Section 3.3). (In contrast, a “regular” append is merely a write at an offset that the client believes to be the current end of file.) The offset is returned to the client and marks the beginning of a defined region that contains the record. In addition, GFS may insert padding or record duplicates in between. They occupy regions considered to be inconsistent and are typically dwarfed by the amount of user data.</p><p>数据变更可能是写入或者是记录追加。写入会将数据写入到应用程序指定的文件偏移位置。记录追加会使数据（也即记录record）至少原子性的追加一次，即使是在并发变更的情况下，但是偏移位置是由GFS决定的（相比之下，「常规」追加只是一次客户端认为的文件当前结尾偏移处的写入操作）。偏移量返回给客户端，并且标记包含追加记录的定义良好的区域的开始位置。此外，GFS可能会在它们之间插入填充或者是记录副本。这些插入的内容会占据被认为是不一致的区域，通常它们比用户数据小很多。</p><p>After a sequence of successful mutations, the mutated file region is guaranteed to be defined and contain the data written by the last mutation. GFS achieves this by </p><ul><li>(a) applying mutations to a chunk in the same order on all its replicas(Section 3.1),</li><li>and (b) using chunk version numbers to detect any replica that has become stale because it has missed mutations while its chunk server was down (Section 4.5). Stale replicas will never be involved in a mutation or given to clients asking the master for chunk locations. They are garbage collected at the earliest opportunity.</li></ul><p>在一系列成功的变更后，文件变更区域会被保证是定义良好的，并且包含最后一次变更写入的数据。GFS通过以下方式来实现：</p><ul><li>将对一个Chunk的变更以同样的顺序应用到该Chunk的所有副本中；</li><li>使用Chunk版本号来检测那些由于Chunk Server 宕机而错过变更数据的陈旧副本。陈旧的副本将不会在参与数据变更或者向客户端响应请求。它们会优先参与垃圾回收。</li></ul><p>Since clients cache chunk locations, they may read from a stale replica before that information is refreshed. This window is limited by the cache entry’s timeout and the next open of the file, which purges from the cache all chunk information for that file. Moreover, as most of our files are append-only, a stale replica usually returns a premature end of chunk rather than outdated data. When a reader retries and contacts the master, it will immediately get current chunk locations.</p><p>客户端会缓存Chunk的位置信息，因此在信息刷新前，它们可能会从那些陈旧的副本中读取数据。此时间窗口会被缓存条目的超时时间以及下次打开文件的限制，这种文件的打开会使缓存清除掉所有该文件的Chunk信息。此外，因为我们的文件通常只是会被追加数据的，所以一个陈旧的副本通常返回的是一个提前结束的Chunk，而不是一个过时的数据。当一个读取者重试并且联系Master时，它会立即得到该Chunk当前的位置信息。</p><p>Long after a successful mutation, component failures can of course still corrupt or destroy data. GFS identifies failed chunk servers by regular handshakes between master and all chunk servers and detects data corruption by checksumming (Section 5.2). Once a problem surfaces, the data is restored from valid replicas as soon as possible (Section 4.3). A chunk is lost irreversibly only if all its replicas are lost before GFS can react, typically within minutes. Even in this case, it becomes unavailable, not corrupted: applications receive clear errors rather than corrupt data.</p><p>在数据变更很久之后，组件的故障仍然可能会损害或破坏数据。GFS通过定期的Master和所有Chunk Server之间的「握手」，来识别发生故障的Chunk Server，并且通过「检验和」来检测数据的损坏。一旦发生问题，数据将会尽快从可用副本中恢复。只有在Master反应之前丢失掉所有的Chunk副本（通常是几分钟以内），Chunk才会出现不可逆的丢失。即使是在这种情况，也只会发生Chunk的不可用而不是数据的损坏：应用程序会收到错误信息，而不是损坏的数据。</p><h3 id="2-7-2-Implications-for-Applications"><a href="#2-7-2-Implications-for-Applications" class="headerlink" title="2.7.2 Implications for Applications"></a>2.7.2 Implications for Applications</h3><p>GFS applications can accommodate the relaxed consistency model with a few simple techniques already needed for other purposes: relying on appends rather than overwrites, checkpointing, and writing self-validating, self-identifying records.</p><p>GFS应用程序可以通过已经被其他目的所需要的简单技术来实现这种宽松的一致性模型，例如：依赖于追加而不是覆盖写操作，检查点，写入时自我验证，自我标识记录。</p><h1 id="3-System-Interactions"><a href="#3-System-Interactions" class="headerlink" title="3 System Interactions"></a>3 System Interactions</h1><p>We designed the system to minimize the master’s involvement in all operations. With that background, we now describe how the client, master, and chunk servers interact to implement data mutations, atomic record append, and snapshot.</p><p>我们以减少Master参与所有操作的目的来设计这个系统。在这个背景之下，我们现在来描述客户端、Master以及Chunk Server之间是如何交互的以实现：数据变更、原子性的记录追加和快照。</p><h2 id="3-1-Leases-and-Mutation-Order"><a href="#3-1-Leases-and-Mutation-Order" class="headerlink" title="3.1 Leases and Mutation Order"></a>3.1 Leases and Mutation Order</h2><p>A mutation is an operation that changes the contents or metadata of a chunk such as a write or an append operation. Each mutation is performed at all the chunk’s replicas. We use leases to maintain a consistent mutation order across replicas. The master grants a chunk lease to one of the replicas, which we call the primary. The primary picks a serial order for all mutations to the chunk. All replicas follow this order when applying mutations. Thus, the global mutation order is defined first by the lease grant order chosen by the master, and within a lease by the serial numbers assigned by the primary.</p><p>变更是一种改变Chunk内容或元数据的操作，例如写和追加操作。每一个变更都会应用到相应Chunk的所有副本之中。我们使用租约来保证所有副本之间变更顺序的一致性。Master向其中一个包含指定副本的Chunk Server授予Chunk租约，这时我们将其称为主副本。主副本会为Chunk的所有变更操作制定一个串型化的顺序。所有的副本都会按照这个顺序来应用变更操作。因此，全局的变更操作的顺序首先由Master选择的租约授予顺序所决定，而同一个租约内的变更顺序则是由选择的主副本定义的。</p><p>The lease mechanism is designed to minimize management overhead at the master. A lease has an initial timeout of 60 seconds. However, as long as the chunk is being mutated, the primary can request and typically receive extensions from the master indefinitely. These extension requests and grants are piggybacked on the HeartBeat messages regularly exchanged between the master and all chunk servers. The master may sometimes try to revoke a lease before it expires (e.g., when the master wants to disable mutations on a file that is being renamed). Even if the master loses communication with a primary, it can safely grant a new lease to another replica after the old lease expires.</p><p>租约机制旨在最大程度的减少Master的管理开销。租约的初始超时时间为60秒。然而，只要Chunk正在被变更，选择的主副本就可以一直向Master请求延长租约。这些延长租约的请求和响应授权通过Master和Chunk Server之间周期交换的心跳报文来传送。Master有时也会在租约到期之前撤销租约（例如，Master想要禁用一个正在重命名的文件上的变更时）。即使Master与主副本断联了，Master也可以在旧的租约到期之后安全的将租约授予给另一个副本。</p><p>In Figure 2, we illustrate this process by following the control flow of a write through these numbered steps.</p><ol><li>The client asks the master which chunk server holds the current lease for the chunk and the locations of the other replicas. If no one has a lease, the master grants one to a replica it chooses (not shown).</li><li>The master replies with the identity of the primary and the locations of the other (secondary) replicas. The client caches this data for future mutations. It needs to contact the master again only when the primary becomes unreachable or replies that it no longer holds a lease.</li><li>The client pushes the data to all the replicas. A client can do so in any order. Each chunk server will store the data in an internal LRU buffer cache until the data is used or aged out. By decoupling the data flow from the control flow, we can improve performance by scheduling the expensive data flow based on the network topology regardless of which chunk server is the primary. Section 3.2 discusses this further.</li><li>Once all the replicas have acknowledged receiving the data, the client sends a write request to the primary. The request identifies the data pushed earlier to all of the replicas. The primary assigns consecutive serial numbers to all the mutations it receives, possibly from multiple clients, which provides the necessary serialization. It applies the mutation to its own local state in serial number order.</li><li>The primary forwards the write request to all secondary replicas. Each secondary replica applies mutations in the same serial number order assigned by the primary.</li><li>The secondaries all reply to the primary indicating that they have completed the operation.</li><li>The primary replies to the client. Any errors encountered at any of the replicas are reported to the client. In case of errors, the write may have succeeded at the primary and an arbitrary subset of the secondary replicas. (If it had failed at the primary, it would not<br>have been assigned a serial number and forwarded.)The client request is considered to have failed, and the modified region is left in an inconsistent state. Our client code handles such errors by retrying the failed mutation. It will make a few attempts at steps (3)through (7) before falling back to a retry from the beginning of the write.</li></ol><p>在图2中我们将通过步骤的编号来表示一个写操作的控制流程：</p><ol><li>客户端会向Master询问哪个Chunk Server获取到了指定Chunk的当前租约，以及其他副本的位置信息。如果没有Chunk Server获取到租约，则Master会将租约授予到其选择的一个副本。</li><li>Master回复主副本的标识，以及其他副本的位置。客户端会缓存此数据，以在将来数据变更时使用。只有当主副本不可达或者回复不再持有租约时，客户端才会需要再次联系Master。</li><li>客户端会将数据推送到所有的副本。客户端可以按照任何顺序执行此操作。每个Chunk Server会将该数据存储到其LRU缓冲区缓存中，直到数据被使用或者超时。通过将数据流和控制流解耦，我们可以通过基于网络拓扑来调动昂贵的数据流，而不管哪个Chunk Server是主副本。3.2节将会进一步讨论这些。</li><li>一旦所有的副本都确认接收到数据，客户端就会向主副本发送写请求。该写请求会标识之前推送到所有副本的数据。主副本会将其接收到的所有变更安排一个连续的序列号来提供必要的串型化，这些变更操作可能来自多个客户端。它会将所有变更按照序列号应用到本地的副本上。</li><li>主副本会将写请求向前传递给所有的次副本。次副本将会按照主副本指定的同样的序列号顺序将所有变更应用到本地。</li><li>次副本会响应主副本以暗示它们已经完成这些操作。</li><li>主副本响应客户端。所有副本所遇到的错误信息都会向客户端报告。在出现错误时，写操作可能已经成功应用到主副本和一些次副本。（如果在主副本上已经出现错误，它将不会再把序列号信息发送给其他次副本）客户端请求将被认为是失败的，修改过的区域将会处于不一致的状态。我们的客户端代码将会通过重试这些变更来处理遇到的错误。他将会先在步骤3到7之间尝试几次后重试这个写操作。</li></ol><h2 id="3-2-Data-Flow"><a href="#3-2-Data-Flow" class="headerlink" title="3.2 Data Flow"></a>3.2 Data Flow</h2><p>We decouple the flow of data from the flow of control to use the network efficiently. While control flows from the client to the primary and then to all secondaries, data is pushed linearly along a carefully picked chain of chunk servers in a pipelined fashion. Our goals are to fully utilize each machine’s network bandwidth, avoid network bottlenecks and high-latency links, and minimize the latency to push through all the data.</p><p>我们将控制流与数据流分离，以高效的利用网络。当控制流从客户端流向主副本，然后会流向所有的次副本，数据流将会以流水线方式按照精心挑选的Chunk Server链线性推送。我们的目标是充分利用每个机器的网络带宽，避免网络瓶颈和高延迟链路，并最大程度的减少推送数据的延迟。</p><p>To fully utilize each machine’s network bandwidth, the data is pushed linearly along a chain of chunk servers rather than distributed in some other topology (e.g., tree). Thus, each machine’s full outbound bandwidth is used to transfer the data as fast as possible rather than divided among multiple recipients.</p><p>为了充分利用每个机器的带宽，数据晚照Chunk Server链线性推送，而不是其他分散的拓扑（例如：树）。因此，每个机器的带宽可以尽可能的全部用来传输数据而不是为多个接受者进行划分。</p><p>To avoid network bottlenecks and high-latency links (e.g., inter-switch links are often both) as much as possible, each machine forwards the data to the “closest” machine in the network topology that has not received it.</p><p>为了尽可能避免网络瓶颈以及高延迟链路，每个机器会将数据推送到在网络拓扑中没收到数据且离它最近的机器。</p><p>Finally, we minimize latency by pipelining the data transfer over TCP connections. Once a chunk server receives some data, it starts forwarding immediately. Pipelining is especially helpful to us because we use a switched network with full-duplex links. Sending the data immediately does not reduce the receive rate.</p><p>最后，我们通过流水线化TCP连接上的数据传输来最小化延迟。Chunk Server一旦接收到数据将会立即传送。流水线对我们特别有帮助，因为我们使用了全双工链路的交换网络。立即发送数据并不会降低接收速率。</p><h2 id="3-3-Atomic-Record-Appends"><a href="#3-3-Atomic-Record-Appends" class="headerlink" title="3.3 Atomic Record Appends"></a>3.3 Atomic Record Appends</h2><p>GFS provides an atomic append operation called record append. In a traditional write, the client specifies the offset at which data is to be written. Concurrent writes to the same region are not serializable: the region may end up containing data fragments from multiple clients. In a record append, however, the client specifies only the data. GFS appends it to the file at least once atomically (i.e., as one continuous sequence of bytes) at an offset of GFS’s choosing and returns that offset to the client.</p><p>GFS提供了一种被称为记录追加的原子追加操作。在传统的写入操作中，客户端要指定数据写入的偏移位置。对于同一区域的并发写操作是不能串型化的：区域的末尾可能包含来自多个客户端的数据碎片。然而在记录追加中，客户端只需要指定数据。GFS会将其至少原子性的追加到文件中一次，追加的位置是由GFS选定的。</p><p>Record append is heavily used by our distributed applications in which many clients on different machines append to the same file concurrently. Clients would need additional complicated and expensive synchronization, for example through a distributed lock manager, if they do so with traditional writes. In our workloads, such files often serve as multiple-producer/single-consumer queues or contain merged results from many different clients.</p><p>我们的分布式应用程序中会大量的使用追加操作，不同机器上的大量客户端会并发的追加到同一个文件。如果使用传统的写操作，客户端需要复杂而又昂贵的同步操作，例如通过一个分布式锁管理。在我们的工作负载中，此类文件通常作为多生产者/单消费者队列或包含来自不同客户端的合并结果。</p><p>Record append is a kind of mutation and follows the control flow in Section 3.1 with only a little extra logic at the primary. The client pushes the data to all replicas of the last chunk of the file Then, it sends its request to the primary. The primary checks to see if appending the record to the current chunk would cause the chunk to exceed the maximum size (64 MB). If so, it pads the chunk to the maximum size, tells secondaries to do the same, and replies to the client indicating that the operation should be retried on the next chunk. (Record append is restricted to be at most one-fourth of the maximum chunk size to keep worst-case fragmentation at an acceptable level.) If the record fits within the maximum size, which is the common case, the primary appends the data to its replica, tells the secondaries to write the data at the exact offset where it has, and finally replies success to the client.</p><p>记录追加是一种变更操作，遵循3.1节中提到的控制流，除了在主副本中只需要一点额外的逻辑。客户端将所有数据（直到文件的最后一个Chunk）推送到所有的副本后，它向主副本发送请求。客户端会检查将记录追加到当前Chunk后是否会超过Chunk的最大值（64MB）。如果超过的话，它会填充当前Chunk到最大值，并且告诉其他次副本做同样的操作，然后告诉客户端在下一个Chunk上重复此操作（译者注：也即将此操作转移到另一个满足大小的Chunk上进行操作）（记录追加被严格限制在Chunk最大值的四分之一，以保证产生的最严重的碎片化在可接受的范围内）。如果记录没有超过最大值，则按普通情况处理，主副本将记录追加到它的副本，并且告诉次副本将此数据写到其拥有的确切偏移处（译者注：即写到与主副本相同的位置），最后向客户端回复成功消息。</p><p>If a record append fails at any replica, the client retries the operation. As a result, replicas of the same chunk may contain different data possibly including duplicates of the same record in whole or in part. GFS does not guarantee that all replicas are byte wise identical. It only guarantees that the data is written at least once as an atomic unit.</p><p>如何任何副本追加失败，客户端将会重试此操作。因此，对于同一个Chunk，副本可能会有不同的数据，这些数据可能包含了相同记录的整个或者部分的重复值。GFS并不会保证所有的副本在位级别上保证一致性。它只保证数据在所有副本上至少原子性的写入一次。</p><h2 id="3-4-Snapshot"><a href="#3-4-Snapshot" class="headerlink" title="3.4 Snapshot"></a>3.4 Snapshot</h2><p>The snapshot operation makes a copy of a file or a directory tree (the “source”) almost instantaneously, while minimizing any interruptions of ongoing mutations. Our users use it to quickly create branch copies of huge data sets (and often copies of those copies, recursively), or to checkpoint the current state before experimenting with changes that can later be committed or rolled back easily.</p><p>快照可以快速的创建一个文件或目录树的拷贝，而且能够最小化对于正在执行的变更的中断。我们的用户用它来创建一个大型数据集的分支，或者创建当前状态的检查点以验证稍后将要提交的更改或者快速回滚。</p><p>Like AFS , we use standard copy-on-write techniques to implement snapshots. When the master receives a snapshot request, it first revokes any outstanding leases on the chunks in the files it is about to snapshot. This ensures that any subsequent writes to these chunks will require an interaction with the master to find the lease holder. This will give the master an opportunity to create a new copy of the chunk first.</p><p>像AFS一样，我们使用标准的写时复制技术来实现快照。当Master接收到快照请求，它首先撤销关于快照的文件的有关Chunk的租约。这就确保对于这些Chunk的后续写操作都要先与Master交互以获得租约持有者。这首先给了Master机会去创建对于Chunk的一个新的拷贝。</p><p>After the leases have been revoked or have expired, the master logs the operation to disk. It then applies this log record to its in-memory state by duplicating the metadata for the source file or directory tree. The newly created snapshot files point to the same chunks as the source files.</p><p>当租约被撤销或者到期后，Master将操作记录到磁盘。然后通过复制源文件或目录树的元数据，将日志记录应用到其内存状态。新创建的快照文件和源文件指向相同的块。</p><p>The first time a client wants to write to a chunk C after the snapshot operation, it sends a request to the master to find the current lease holder. The master notices that the reference count for chunk C is greater than one. It defers replying to the client request and instead picks a new chunk handle C’. It then asks each chunk server that has a current replica of C to create a new chunk called C’. By creating the new chunk on the same chunk servers as the original, we ensure that the data can be copied locally, not over the network(our disks are about three times as fast as our 100 Mb Ethernet links). From this point, request handling is no different from that for any chunk: the master grants one of the replicas a lease on the new chunk C’ and replies to the client, which can write the chunk normally, not knowing that it has just been created from an existing chunk.</p><p>客户端在快照操作后第一次想要写入Chunk C时，它向Master发送请求以查询当前合约的持有者。Master注意到Chunk C的引用计数大于1。它延迟向客户端回复请求，而且选择一个新的Chunk Handle C‘。然后它要求所有拥有Chunk C副本的Chunk Server创建一个新的叫做C‘的Chunk。通过在与原始Chunk Server相同的Chunk Server上创建新的Chunk，我们可以确保数据是在本地复制的，而不是通过网络（我们的磁盘速度大概是100MB以太网链路的三倍）。通过这一点，对于任何Chunk的请求处理都没什么不同：master将新创建的Chunk C‘的租约授予给一个副本Chunk Server，然后回复客户端可以正常写入这个Chunk了，客户端不会知道这是刚刚从现有的Chunk中创建出来的副本。</p><h1 id="4-Master-Operation"><a href="#4-Master-Operation" class="headerlink" title="4 Master Operation"></a>4 Master Operation</h1><p>The master executes all namespace operations. In addition, it manages chunk replicas throughout the system: it makes placement decisions, creates new chunks and hence replicas, and coordinates various system-wide activities to keep chunks fully replicated, to balance load across all the chunk servers, and to reclaim unused storage. We now discuss each of these topics.</p><p>Master执行所有的命名空间操作。此外，它还管理整个系统的Chunk副本：它做出放置决策，创建新的Chunk和副本，协调整个系统范围内的活动以保证Chunk被备份，平衡所有Chunk Server之间的负载，以及回收未使用的存储。我们现在将逐个讨论这些话题。</p><h2 id="4-1-Namespace-Management-and-Locking"><a href="#4-1-Namespace-Management-and-Locking" class="headerlink" title="4.1 Namespace Management and Locking"></a>4.1 Namespace Management and Locking</h2><p>Many master operations can take a long time: for example, a snapshot operation has to revoke chunk server leases on all chunks covered by the snapshot. We do not want to delay other master operations while they are running. Therefore, we allow multiple operations to be active and use locks over regions of the namespace to ensure proper serialization.</p><p>许多的Master操作需要花费很长的时间：比如，一个快照操作不得不使快照所覆盖的所有的块都撤销其租约。我们并不想延迟其他正在运行的Master操作。因此，我们允许多个操作处于活跃状态，并且在命名空间的区域上使用锁来保证正确的序列化。</p><p>Unlike many traditional file systems, GFS does not have a per-directory data structure that lists all the files in that directory. Nor does it support aliases for the same file or directory (i.e, hard or symbolic links in Unix terms). GFS logically represents its namespace as a lookup table mapping full pathnames to metadata. With prefix compression, this table can be efficiently represented in memory. Each node in the namespace tree (either an absolute file name or an absolute directory name) has an associated read-write lock.</p><p>与许多的传统文件系统不同，GFS并没有一个能列举出目录中所有文件的目录数据结构。此外，它也不支持对于同一个文件和目录的别名（例如Unix系统中的硬链接和符号链接）。GFS在逻辑上将其命名空间表示为将完整的目录名映射到元数据的查找表。通过前缀压缩能有效的在内存中展示该表。命名空间树中的每个节点（包括绝对文件名和绝对目录名）都有一个关联的读写锁。</p><p>Each master operation acquires a set of locks before it runs. Typically, if it involves <code>/d1/d2/.../dn/leaf,</code> it will acquire read-locks on the directory names <code>/d1, /d1/d2, ..., /d1/d2/.../dn</code>, and either a read lock or a write lock on the full pathname <code>/d1/d2/.../dn/leaf</code>.Note that leaf may be a file or directory depending on the operation.</p><p>每个Master的操作在它运行之前都需要获得一个锁的集合。典型的，如果它需要操作<code>/d1/d2/.../dn/leaf</code> ，那么它需要获得在目录<code>/d1,/d1/d2,.../d1/d2/.../dn</code> 上的读锁，以及一个在全路径<code>/d1/d2/..../dn/leaf</code>上的读锁或写锁。需要注意的是，leaf可能是个文件或目录，这取决于具体的操作。</p><p>We now illustrate how this locking mechanism can prevent a file <code>/home/user/foo</code> from being created while <code>/home/user</code> is being snapshotted to <code>/save/user</code>. The snapshot operation acquires read locks on <code>/home</code> and <code>/save</code>, and write locks on <code>/home/user</code> and <code>/save/user</code>. The file creation acquires read locks on <code>/home</code> and <code>/home/user</code>, and a write lock on <code>/home/user/foo</code>. The two operations will be serialized properly because they try to obtain conflicting locks on <code>/home/user</code>. File creation does not require a write lock on the parent directory because there is no “directory”, or <em><code>inode-like</code></em>, data structure to be protected from modification. The read lock on the name is sufficient to protect the parent directory from deletion.</p><p>我们现在来列举锁机制是如何避免<code>/home/user/foo</code>被创建的，当在创建<code>/home/user</code>的快照<code>/save/user</code>时。快照操作需要获得<code>/home</code>和<code>/save</code>的读锁，以及<code>/home/user</code>和<code>/save/user</code>的写锁。文件创建需要获得<code>/home</code>和<code>/home/user</code>的读锁，以及<code>/home/user/foo</code>的写锁。这两个操作将被正确的序列化，因为它们试图获得在<code>/home/user</code>上的冲突锁。文件创建不需要获得副目录的写锁，因为这里并没有目录或者类似<code>inode</code>的数据结构需要被保护以防止修改。读锁已经足够用来保护副目录被删除。</p><p>One nice property of this locking scheme is that it allows concurrent mutations in the same directory. For example, multiple file creations can be executed concurrently in the same directory: each acquires a read lock on the directory name and a write lock on the file name. The read lock on the directory name suffices to prevent the directory from being deleted, renamed, or snapshotted. The write locks on file names serialize attempts to create a file with the same name twice.</p><p>这种锁机制的一个好处是允许同一个目录内的并发变更操作。例如，可以在同一个目录内同时执行多个文件的创建操作：每个创建操作需要一个对于目录名的读锁，以及对于文件名的写锁。目录名的写锁用于放置目录被删除、重命名或被执行快照操作。在文件名上的写锁用于序列化对于同一个文件名的创建操作。</p><p>Since the namespace can have many nodes, read-write lock objects are allocated lazily and deleted once they are not in use. Also, locks are acquired in a consistent total order to prevent deadlock: they are first ordered by level in the namespace tree and lexicographically within the same level.</p><p>由于命名空间可以有多个节点，所以读写锁对象会被惰性分配，一旦不使用就被删除。 此外，以一致的总顺序获取锁以防止死锁：它们首先在命名空间树中按级别排序，并在同一级别内按字典顺序排列。</p><h2 id="4-2-Replica-Placement"><a href="#4-2-Replica-Placement" class="headerlink" title="4.2 Replica Placement"></a>4.2 Replica Placement</h2><p>A GFS cluster is highly distributed at more levels than one. It typically has hundreds of chunk servers spread across many machine racks. These chunk servers in turn may be accessed from hundreds of clients from the same or different racks. Communication between two machines on different racks may cross one or more network switches. Additionally, bandwidth into or out of a rack may be less than the aggregate bandwidth of all the machines within the rack. Multi-level distribution presents a unique challenge to distribute data for scalability, reliability, and availability.</p><p>GFS在各个层级上都实现了高度的分布式。它通常由几百个分布在多个机架上的Chunk Server组成。这些Chunk Server又可能被几百个来自相同或不同机架上的客户端访问。来自不同机架上的两个机器之间的通信可能或跨一个或多个交换机。此外，进出一个机架的带宽可能会小于一个机架上所有机器的总带宽。多层级的分布式也面临着独一无二的挑战：分布式数据的扩展性、可靠性和可用性。</p><p>The chunk replica placement policy serves two purposes: maximize data reliability and availability, and maximize network bandwidth utilization. For both, it is not enough to spread replicas across machines, which only guards against disk or machine failures and fully utilizes each machine’s network bandwidth. We must also spread chunk replicas across racks. This ensures that some replicas of a chunk will survive and remain available even if an entire rack is damaged or offline (for example, due to failure of a shared resource like a network switch or power circuit). It also means that traffic, especially reads, for a chunk can exploit the aggregate bandwidth of multiple racks. On the other hand, write traffic has to flow through multiple racks, a trade off we make willingly.</p><p>Chunk副本的放置策略主要服务于两个目的：最大化数据的可靠性和可用性，最大化利用网络带宽。对于两者来说，仅仅实现跨机器的副本是不够的，这只能保证抵抗磁盘或机器的错误，以及最大化利用每个机器的网络带宽。我们必须实现Chunk副本的跨机架。这能保证一个Chunk的副本是可用的，即使一整个机架都被破坏或者下线（例如，网络交换机和电源电路等共享资源的故障）。这也意味着，对于一个Chunk的流量特别是读操作，可以充分利用多个机架的总带宽。另一方面，写流量需要在多个机架之间进行，这也是我们自愿做出的权衡。</p><h2 id="4-3-Creation-Replication-Rebalancing"><a href="#4-3-Creation-Replication-Rebalancing" class="headerlink" title="4.3 Creation, Replication, Rebalancing"></a>4.3 Creation, Replication, Rebalancing</h2><p>Chunk replicas are created for three reasons: chunk creation, re-replication, and rebalancing.</p><p>Chunk副本的创建有三种原因：Chunk的创建，重备份，重平衡。</p><p>When the master creates a chunk, it chooses where to place the initially empty replicas. It considers several factors. (1) We want to place new replicas on chunk servers with below-average disk space utilization. Over time this will equalize disk utilization across chunk servers. (2) We want to limit the number of “recent” creations on each chunk server. Although creation itself is cheap, it reliably predicts imminent heavy write traffic because chunks are created when demanded by writes, and in our append-once-read-many workload they typically become practically read-only once they have been completely written. (3) As discussed above, we want to spread replicas of a chunk across racks.</p><p>当Master创建一个Chunk的时候，它会选择在何处放置初始化为空的副本。它会考虑以下几个因素：</p><ol><li>我们会希望将新的副本放置在低于平均磁盘利用率的Chunk Serve上。随着时间的推移，这将会平衡各个Chunk Server的磁盘利用率。</li><li>我们希望能限制在每个Chunk Server 上的「最近」创建Chunk的数量。尽管创建本身是比较廉价的，但是这能可靠的预测即将到来的大量的写流量，因为Chunk是为了写操作而创建的，并且在我们的一次写入多次读的负载模型中，一旦写入完成它们通常都是只读的。</li><li>正如我们在上面讨论的那样，我们希望实现Chunk副本的跨机架放置。</li></ol><p>The master re-replicates a chunk as soon as the number of available replicas falls below a user-specified goal. This could happen for various reasons: a chunk server becomes unavailable, it reports that  it‘s replica may be corrupted, one of its disks is disabled because of errors, or the replication goal is increased. Each chunk that needs to be re-replicated is prioritized based on several factors. One is how far it is from its replication goal. For example, we give higher priority to a chunk that has lost two replicas than to a chunk that has lost only one. In addition, we prefer to first re-replicate chunks for live files as opposed to chunks that belong to recently deleted files (see Section 4.4). Finally, to minimize the impact of failures on running applications, we boost the priority of any chunk that is blocking client progress.</p><p>Master会在当Chunk副本的数量少于用户预定义的数量时进行重备份。这可能发生在以下情况：一个Chunk Server变得不可达，它报告自己的副本可能被污染了，它的一个磁盘由于错误变得不可用了，或者预设的副本数量增加了。需要重新备份的Chunk的优先级主要有以下几个因素来确定。一个是它与备份的目标数量差了多少。例如，我们将更高的优先级给丢失了两个副本的Chunk而不是只丢失了一个副本的Chunk。</p><p>The master picks the highest priority chunk and “clones” it by instructing some chunk server to copy the chunk data directly from an existing valid replica. The new replica is placed with goals similar to those for creation: equalizing disk space utilization, limiting active clone operations on any single chunk server, and spreading replicas across racks. To keep cloning traffic from overwhelming client traffic, the master limits the numbers of active clone operations both for the cluster and for each chunk server. Additionally, each chunk server limits the amount of bandwidth it spends on each clone operation by throttling its read requests to the source chunk server.</p><p>Master挑选具有最高优先级的Chunk，并且通过命令其他Chunk Server直接通过其他可用的副本来复制Chunk数据来进行Chunk的克隆操作。这个新的副本的放置目标类似创建操作：平衡磁盘空间利用率，限制对于单个Chunk Server上的活跃克隆操作数量，实现副本的跨机架放置。为了防止克隆流量超过客户端流量，Master 限制了集群和每个Chunk服务器的活跃克隆操作的数量。 此外，每个Chunk服务器通过限制对源Chunk服务器的读取请求来限制它在每个克隆操作上花费的带宽量。</p><p>Finally, the master rebalances replicas periodically: it examines the current replica distribution and moves replicas for better disk space and load balancing. Also through this process, the master gradually fills up a new chunk server rather than instantly swamps it with new chunks and the heavy write traffic that comes with them. The placement criteria for the new replica are similar to those discussed above. In addition, the master must also choose which existing replica to remove. In general, it prefers to remove those on chunk servers with below-average free space so as to equalize disk space usage.</p><p>最后，Master会周期性的重平衡副本：它会检验当前副本的分布，移动副本以实现更好的磁盘空间和负载的平衡。通过这个过程，Master会逐渐填满一个新的Chunk Server，而不是将大量的新Chunk和随之而来的写流量来淹没它。新副本的放置标准和我们之前讨论的类似。此外，Master还必须要选择删除哪个现有的副本。通常，它更偏向于删除那些位于低于平均空闲空间Chunk Server上的副本，以平衡磁盘上的可用空间。</p><h2 id="4-4-Garbage-Collection"><a href="#4-4-Garbage-Collection" class="headerlink" title="4.4 Garbage Collection"></a>4.4 Garbage Collection</h2><p>After a file is deleted, GFS does not immediately reclaim the available physical storage. It does so only lazily during regular garbage collection at both the file and chunk levels. We find that this approach makes the system much simpler and more reliable.</p><p>文件被删除后，GFS并不会立即回收可用的物理存储。它只会在文件和Chunk级别上的常规垃圾回收期间惰性的执行这样的操作。我们发现这样可以使系统更简单和可靠。</p><h3 id="4-4-1-Mechanism"><a href="#4-4-1-Mechanism" class="headerlink" title="4.4.1 Mechanism"></a>4.4.1 Mechanism</h3><p>When a file is deleted by the application, the master logs the deletion immediately just like other changes. However instead of reclaiming resources immediately, the file is just renamed to a hidden name that includes the deletion times-tamp. During the master’s regular scan of the file system namespace, it removes any such hidden files if they have existed for more than three days (the interval is configurable). Until then, the file can still be read under the new, special name and can be undeleted by renaming it back to normal. When the hidden file is removed from the namespace, its in-memory metadata is erased. This effectively severs its links to all its chunks.</p><p>当一个文件被应用程序删除后，Master会像记录其他更改一样立刻记录删除操作。然而，文件只是被重命名为一个包含了删除时间戳的隐藏名称，而不是立刻回收资源。在Master定期扫描系统命名空间时，它会删除那些存在超过三天的隐藏文件（时间间隔是可配置的）。在此之前，仍可以使用新的特殊名称读取该文件，并且可以通过将其重命名为正常名称来取消删除该文件。 当隐藏文件从命名空间中移除时，其内存中的元数据将被擦除。 这有效地切断了它与所有块的链接。</p><p>In a similar regular scan of the chunk namespace, the master identifies orphaned chunks (i.e., those not reachable from any file) and erases the metadata for those chunks. In a HeartBeat message regularly exchanged with the master, each chunk server reports a subset of the chunks it has, and the master replies with the identity of all chunks that are no longer present in the master’s metadata. The chunk server is free to delete its replicas of such chunks. </p><p>在类似的Chunk命名空间的定期扫描中，Master会识别孤儿Chunk（例如那些不被任何文件可达的Chunk）并且擦除这些Chunk的元数据。在定期与Master交换的心跳报文中，每个Chunk Server都会报告它所拥有的Chunk的子集，Master会回复它已经没有其元数据的所有Chunk的标识。Chunk Server可以自由的删除这些块的副本。</p><h3 id="4-4-2-Discussion"><a href="#4-4-2-Discussion" class="headerlink" title="4.4.2 Discussion"></a>4.4.2 Discussion</h3><p>Although distributed garbage collection is a hard problem that demands complicated solutions in the context of programming languages, it is quite simple in our case. We can easily identify all references to chunks: they are in the file-to-chunk mappings maintained exclusively by the master. We can also easily identify all the chunk replicas: they are Linux files under designated directories on each chunk server. Any such replica not known to the master is “garbage.”</p><p>尽管分布式垃圾回收在编程语言的上下文中是一个需要复杂解决方案的难题，但在我们的案例中却非常简单。 我们可以很容易地识别出所有对Chunk的引用：它们位于由Master专门维护的文件到块的映射中。 我们还可以轻松识别所有Chunk副本：它们是每个Chunk Server上指定目录下的 Linux 文件。 Master不知道的任何此类副本都是“垃圾”。</p><p>The garbage collection approach to storage reclamation offers several advantages over eager deletion. First, it is simple and reliable in a large-scale distributed system where component failures are common. Chunk creation may succeed on some chunk servers but not others, leaving replicas that the master does not know exist. Replica deletion messages may be lost, and the master has to remember to resend them across failures, both its own and the chunk server’s. Garbage collection provides a uniform and dependable way to clean up any replicas not known to be useful. Second, it merges storage reclamation into the regular background activities of the master, such as the regular scans of namespaces and handshakes with chunk servers. Thus, it is done in batches and the cost is amortized. Moreover, it is done only when the master is relatively free. The master can respond more promptly to client requests that demand timely attention. Third, the delay in reclaiming storage provides a safety net against accidental, irreversible deletion.</p><p>与立刻删除相比，存储回收的垃圾回收方法提供了几个优点。首先，它在组件故障常见的大型分布式系统中简单可靠。Chunk创建可能在某些Chunk Server上成功但在其他Chunk Server上不会成功，从而留下Master不知道存在的副本。副本删除消息可能会丢失，并且 Master 必须记住在失败时重新发送它们，包括它自己的和Chunk Server的。垃圾收集提供了一种统一且可靠的方法来清理任何已知无用的副本。其次，它将存储回收合并到 Master 的常规后台活动中，例如命名空间的常规扫描和与Chunk Server的握手。因此，它是分批完成的，成本被摊销。而且，只有在Master比较空闲的时候才做。 Master 可以更迅速地响应需要及时关注的客户端请求。第三，回收存储的延迟提供了防止意外、不可逆删除的安全网。</p><h2 id="4-5-Stale-Replica-Detection"><a href="#4-5-Stale-Replica-Detection" class="headerlink" title="4.5 Stale Replica Detection"></a>4.5 Stale Replica Detection</h2><p>Chunk replicas may become stale if a chunk server fails and misses mutations to the chunk while it is down. For each chunk, the master maintains a chunk version number to distinguish between up-to-date and stale replicas.</p><p>如果Chunk Server发生故障并且在它关闭时错过了对Chunk的变更，则Chunk副本可能会变得过时。 对于每个Chunk，Master都会维护一个Chunk版本号以区分最新和陈旧的副本。</p><p>Whenever the master grants a new lease on a chunk, it increases the chunk version number and informs the up-to-date replicas. The master and these replicas all record the new version number in their persistent state. This occurs before any client is notified and therefore before it can start writing to the chunk. If another replica is currently unavailable, its chunk version number will not be advanced. The master will detect that this chunk server has a stale replica when the chunk server restarts and reports its set of chunks and their associated version numbers. If the master sees a version number greater than the one in its records, the master assumes that it failed when granting the lease and so takes the higher version to be up-to-date.</p><p>每当Master授予一个Chunk新的租约时，它就会增加Chunk版本号并通知最新的副本。 Master 和这些副本都在它们的持久状态中记录了新的版本号。 这发生在任何客户端被通知之前，因此在它可以开始写入Chunk之前。 如果另一个副本当前不可用，则其Chunk版本号不会继续增加。 当Chunk Server重新启动并报告其Chunk集合及其相关联的版本号时，Master将检测到该Chunk Server具有过时的副本。 如果 Master 看到版本号大于其记录中的版本号，则 Master 假定它在授予租约时失败，因此将更高的版本更新为最新版本。</p><h1 id="5-Fault-Tolerance-And-Diagnosis"><a href="#5-Fault-Tolerance-And-Diagnosis" class="headerlink" title="5 Fault Tolerance And Diagnosis"></a>5 Fault Tolerance And Diagnosis</h1><p>One of our greatest challenges in designing the system is dealing with frequent component failures. The quality and quantity of components together make these problems more the norm than the exception: we cannot completely trust the machines, nor can we completely trust the disks. Component failures can result in an unavailable system or, worse, corrupted data. We discuss how we meet these challenges and the tools we have built into the system to diagnose problems when they inevitably occur.</p><p>我们在设计系统时遇到的最大的挑战之一是处理频繁的组件故障。组件的质量和数量共同导致这些问题成为常态而不是意外：我们既不能完全信任这些机器，也不能完全信任这些磁盘。组件故障会导致系统不可用，甚至更严重的是会导致数据的损坏。我们讨论了我们是如何应对这些挑战的，以及我们在系统中内置的工具，以便在问题不可避免地发生时进行诊断。</p><h2 id="5-1-High-Availability"><a href="#5-1-High-Availability" class="headerlink" title="5.1 High Availability"></a>5.1 High Availability</h2><p>Among hundreds of servers in a GFS cluster, some are bound to be unavailable at any given time. We keep the overall system highly available with two simple yet effective strategies: fast recovery and replication.</p><p>在GFS的数百台机器中，在任何给定的时间总有些机器是不可用的。我们通过两个简单却有效的策略来保证整个系统的高可用性：快速恢复和备份。</p><h3 id="5-1-1-Fast-Recovery"><a href="#5-1-1-Fast-Recovery" class="headerlink" title="5.1.1 Fast Recovery"></a>5.1.1 Fast Recovery</h3><p>Both the master and the chunk server are designed to restore their state and start in seconds no matter how they terminated. In fact, we do not distinguish between normal and abnormal termination; servers are routinely shut down just by killing the process. Clients and other servers experience a minor hiccup as they time out on their outstanding requests, reconnect to the restarted server, and retry. Section 6.2.2 reports observed startup times.</p><p>Master和Chunk Server都被设计为恢复它们的状态和在几秒后重启而不管他们是如何被终止的。实际上，我们并不会区分正常和异常终止。服务器会例行的通过杀死进程来关闭。客户端和其他服务器在处理未完成的请求时会遇到小问题，重新连接到重新启动的服务器并重试。 第 6.2.2 节报告观察到的启动时间。</p><h3 id="5-1-2-Chunk-Replication"><a href="#5-1-2-Chunk-Replication" class="headerlink" title="5.1.2 Chunk Replication"></a>5.1.2 Chunk Replication</h3><p>As discussed earlier, each chunk is replicated on multiple chunk servers on different racks. Users can specify different replication levels for different parts of the file namespace. The default is three. The master clones existing replicas as needed to keep each chunk fully replicated as chunk servers go offline or detect corrupted replicas through checksum verification (see Section 5.2). Although replication has served us well, we are exploring other forms of cross-server redundancy such as parity or erasure codes for our increasing read- only storage requirements. We expect that it is challenging but manageable to implement these more complicated redundancy schemes in our very loosely coupled system be- cause our traffic is dominated by appends and reads rather than small random writes.</p><p>正如前面讨论的那样，每个Chunk被复制到多个位于不同机架上的Chunk Server上。用户可以为文件命名空间的不同部分设置不同的备份级别。默认为3。</p><h3 id="5-1-3-Master-Replication"><a href="#5-1-3-Master-Replication" class="headerlink" title="5.1.3 Master Replication"></a>5.1.3 Master Replication</h3><p>The master state is replicated for reliability. Its operation log and checkpoints are replicated on multiple machines. A mutation to the state is considered committed only after its log record has been flushed to disk locally and on all master replicas. For simplicity, one master process remains in charge of all mutations as well as background activities such as garbage collection that change the system internally. When it fails, it can restart almost instantly. If its machine or disk fails, monitoring infrastructure outside GFS starts a new master process elsewhere with the replicated operation log. Clients use only the canonical name of the master (e.g. gfs-test), which is a DNS alias that can be changed if the master is relocated to another machine.</p><p>Master的状态被备份以保证可靠性。它的操作日志和检查点被复制到多台机器上。对于状态的变更只有当它的日志记录被刷新到本地磁盘和所有的Master副本上后才会被确认提交。为了简单起见，一个Master进程负责处理所有的变更和后台活动，就像更改内部系统的垃圾回收那样。当出现故障时，它几乎可以立即重启。如果它的机器或者磁盘出现故障，GFS之外的监控基础设施会在其他地方重新启动一个带有复制的操作日志的新的Master进程。客户端仅使用Master 的规范名称（例如 gfs-test），这是一个 DNS 别名，如果 master 重新定位到另一台机器，则可以更改该别名。</p><p>Moreover, “shadow” masters provide read-only access to the file system even when the primary master is down. They are shadows, not mirrors, in that they may lag the primary slightly, typically fractions of a second. They enhance read availability for files that are not being actively mutated or applications that do not mind getting slightly stale results. In fact, since file content is read from chunk servers, applications do not observe stale file content. What could be stale within short windows is file metadata, like directory contents or access control information.</p><p>此外，「影子」Master提供对文件系统的只读访问，即使主Master宕掉。它们是影子，而不是镜子，因为它们可能稍微滞后于主Master，通常是几分之一秒。 它们增强了未主动变更的文件或不介意获得稍微过时的结果的应用程序的读取可用性。 事实上，由于文件内容是从Chunk Server读取的，应用程序不会观察到陈旧的文件内容。 在短窗口中可能过时的是文件元数据，如目录内容或访问控制信息。</p><p>To keep itself informed, a shadow master reads a replica of the growing operation log and applies the same sequence of changes to its data structures exactly as the primary does. Like the primary, it polls chunk servers at startup (and infrequently thereafter) to locate chunk replicas and exchanges frequent handshake messages with them to monitor their status. It depends on the primary master only for replica location updates resulting from the primary’s decisions to create and delete replicas.</p><p>为了保证它被通知，影子Master读取不断增长的操作日志的副本，并且按照和主Master同样的更改序列应用到它的数据结构。与主Master一样，它在启动时（之后很少）轮询Chunk Server以定位Chunk副本并与它们交换频繁的握手消息以监控它们的状态。 它仅依赖于主Master来更新由主Master创建和删除副本的决定所导致的副本位置更新。</p><h2 id="5-2-Data-Integrity"><a href="#5-2-Data-Integrity" class="headerlink" title="5.2 Data Integrity"></a>5.2 Data Integrity</h2><p>Each chunk server uses checksumming to detect corruption of stored data. Given that a GFS cluster often has thousands of disks on hundreds of machines, it regularly experiences disk failures that cause data corruption or loss on both the read and write paths. (See Section 7 for one cause.) We can recover from corruption using other chunk replicas, but it would be impractical to detect corruption by comparing replicas across chunk servers. Moreover, divergent replicas may be legal: the semantics of GFS mutations, in particular atomic record append as discussed earlier, does not guarantee identical replicas. Therefore, each chunk server must independently verify the integrity of its own copy by maintaining checksums.</p><p>每个Chunk Server使用检验和来检测存储数据的损坏。因为每个GFS集群通常在几百台机器上有几千个磁盘，这会经常遇到磁盘故障，并导致在读或写路径上的数据损坏和丢失。我们可以使用其他的Chunk副本来从损坏中恢复数据，但通过跨Chunk Server的方式来比较副本以检验数据损坏是不切实际的。此外，不同的副本可能是合法的：GFS变更的语义，特别是在我们之前讨论过的原子追加记录上，并不会保证一致的副本。因此，每个Chunk Server必须通过维护独立检验和的方式来保证它自己副本数据的完整性。</p><p>A chunk is broken up into 64 KB blocks. Each has a corresponding 32 bit checksum. Like other metadata, checksums are kept in memory and stored persistently with logging, separate from user data.</p><p>每个Chunk被划分为64KB大小的块。每个块都有对应的32位的检验和。像元数据一样，检验和被保存在内存中，并通过日志的方式持久化，独立于用户数据。</p><p>For reads, the chunk server verifies the checksum of data blocks that overlap the read range before returning any data to the requester, whether a client or another chunk server. Therefore chunk servers will not propagate corruptions to other machines. If a block does not match the recorded checksum, the chunk server returns an error to the requestor and reports the mismatch to the master. In response, the requestor will read from other replicas, while the master will clone the chunk from another replica. After a valid new replica is in place, the master instructs the chunk server that reported the mismatch to delete its replica.</p><p>对于读操作，Chunk Server在将数据返回给请求者之前（无论是客户端还是其他的Chunk Server），都会验证读操作范围内的数据块的检验和。因此，Chunk Server不会将损坏的数据传播到其他机器。如果一个块不匹配记录的检验和，Chunk Server将会向请求者返回一个错误信息，并且向Master报告不匹配信息。作为响应，请求者将会从其他的副本读取数据，并且Master将会从其他的副本克隆Chunk。在有效的新副本就位后，Master会指示报告不匹配的Chunk Server删除其副本。</p><p>Checksumming has little effect on read performance for several reasons. Since most of our reads span at least a few blocks, we need to read and checksum only a relatively small amount of extra data for verification. GFS client code further reduces this overhead by trying to align reads at checksum block boundaries. Moreover, checksum lookups and comparison on the chunk server are done without any I/O, and checksum calculation can often be overlapped with I/Os.</p><p>出于多个原因，检验和对读操作的性能机会没有影响。因为我们的读操作至少跨越几个块，我们只需要读取和检验相对很少的额外数据进行验证。GFS客户端代码尝试通过在检验和的块边界对齐读取以进一步减少该开销。此外，Chunk Server上的检验和查找和比较是在没有任何IO的情况下完成的，并且检验和的计算通常会与IO重叠。</p><p>Checksum computation is heavily optimized for writes that append to the end of a chunk(as opposed to writes that overwrite existing data) because they are dominant in our workloads. We just incrementally update the checksum for the last partial checksum block, and compute new checksums for any brand new checksum blocks filled by the append. Even if the last partial checksum block is already corrupted and we fail to detect it now, the new checksum value will not match the stored data, and the corruption will be detected as usual when the block is next read.</p><p>检验和计算针对附加到块的末尾的写入（而不是覆盖现有数据的写入）因为在我们的工作负载中它们占主导地位。我们只会增量更新最后部分检验和块的检验和，并为由追加填充得到的新的检验和块计算新的检验和。即使最后的部分检验和块已经损坏了，而且我们现在没法检测到它，新的检验和值也不会匹配到存储的数据，并且会在下次读取块时检测到损坏。</p><p>In contrast, if a write overwrites an existing range of the chunk, we must read and verify the first and last blocks of the range being overwritten, then perform the write, and finally compute and record the new checksums. If we do not verify the first and last blocks before overwriting them partially, the new checksums may hide corruption that exists in the regions not being overwritten.</p><p>相反，如果写入操作覆盖写了Chunk的现有范围，我们必须读取和检测覆盖写范围内的第一个和最后一个块，然后执行写入，最后计算并记录新的检验和。如果我们不对部分覆盖写范围内的第一个和最后一个块进行验证，新的检验和可能会隐藏存在于覆盖写范围之外的损坏。</p><p>During idle periods, chunk servers can scan and verify the contents of inactive chunks. This allows us to detect corruption in chunks that are rarely read. Once the corruption is detected, the master can create a new uncorrupted replica and delete the corrupted replica. This prevents an inactive but corrupted chunk replica from fooling the master into thinking that it has enough valid replicas of a chunk.</p><p>在空闲时期，Chunk Server会扫描和验证非活动Chunk的内容。这使我们可以检测到那些很少读取的Chunk中的损坏。一旦检测到损坏，Master就可以创建一个新的未损坏的副本并删除已损坏的副本。这避免了不活跃但已损坏的Chunk副本欺骗Master，使其以为我们已经拥有了足够的可用Chunk副本。</p><h2 id="5-3-Diagnostic-Tools"><a href="#5-3-Diagnostic-Tools" class="headerlink" title="5.3 Diagnostic Tools"></a>5.3 Diagnostic Tools</h2><p>Extensive and detailed diagnostic logging has helped immeasurably in problem isolation, debugging, and performance analysis, while incurring only a minimal cost. Without logs, it is hard to understand transient, non-repeatable interactions between machines. GFS servers generate diagnostic logs that record many significant events (such as chunk servers going up and down) and all RPC requests and replies. These diagnostic logs can be freely deleted without affecting the correctness of the system. However, we try to keep these logs around as far as space permits.</p><p>广泛而详细的诊断日志在问题隔离、调试和性能分析方面起到了不可估量的作用，同时只产生了最低限度的成本。 没有日志，就很难理解机器之间短暂的、不可重复的交互。 GFS 服务器生成诊断日志，来记录许多重要事件（例如Chunk Server启动和关闭）以及所有 RPC 请求和回复。 这些诊断日志可以随意删除，不影响系统的正确性。 但是，我们会尽量在空间允许的范围内保留这些日志。</p><p>The RPC logs include the exact requests and responses sent on the wire, except for the file data being read or written. By matching requests with replies and collating RPC records on different machines, we can reconstruct the entire interaction history to diagnose a problem. The logs also serve as traces for load testing and performance analysis.</p><p>RPC 日志包括在线路上发送的确切请求和响应，但正在读取或写入的文件数据除外。 通过将请求与回复匹配并整理不同机器上的 RPC 记录，我们可以重建整个交互历史以诊断问题。 日志还用作负载测试和性能分析的跟踪。</p><p>The performance impact of logging is minimal (and far outweighed by the benefits) because these logs are written sequentially and asynchronously. The most recent events are also kept in memory and available for continuous online monitoring.</p><p>日志记录对性能的影响很小（并且远远超过好处），因为这些日志是按顺序和异步写入的。 最近的事件也保存在内存中，可用于持续在线监控。</p><h1 id="6-Measurements"><a href="#6-Measurements" class="headerlink" title="6 Measurements"></a>6 Measurements</h1><p>以下内容略。</p><h2 id="6-1-Micro-benchmarks"><a href="#6-1-Micro-benchmarks" class="headerlink" title="6.1 Micro-benchmarks"></a>6.1 Micro-benchmarks</h2><h3 id="6-1-1-Reads"><a href="#6-1-1-Reads" class="headerlink" title="6.1.1 Reads"></a>6.1.1 Reads</h3><h3 id="6-1-2-Writes"><a href="#6-1-2-Writes" class="headerlink" title="6.1.2 Writes"></a>6.1.2 Writes</h3><h3 id="6-1-3-Record-Appends"><a href="#6-1-3-Record-Appends" class="headerlink" title="6.1.3 Record Appends"></a>6.1.3 Record Appends</h3><h2 id="6-2-Real-World-Cluster"><a href="#6-2-Real-World-Cluster" class="headerlink" title="6.2 Real World Cluster"></a>6.2 Real World Cluster</h2><h3 id="6-2-1-Storage"><a href="#6-2-1-Storage" class="headerlink" title="6.2.1 Storage"></a>6.2.1 Storage</h3><h3 id="6-2-2-Metadata"><a href="#6-2-2-Metadata" class="headerlink" title="6.2.2 Metadata"></a>6.2.2 Metadata</h3><h3 id="6-2-3-Read-and-Write-Rates"><a href="#6-2-3-Read-and-Write-Rates" class="headerlink" title="6.2.3 Read and Write Rates"></a>6.2.3 Read and Write Rates</h3><h3 id="6-2-4-Master-Load"><a href="#6-2-4-Master-Load" class="headerlink" title="6.2.4 Master Load"></a>6.2.4 Master Load</h3><h3 id="6-2-5-Recovery-Time"><a href="#6-2-5-Recovery-Time" class="headerlink" title="6.2.5 Recovery Time"></a>6.2.5 Recovery Time</h3><h2 id="6-3-Workload-Breakdown"><a href="#6-3-Workload-Breakdown" class="headerlink" title="6.3 Workload Breakdown"></a>6.3 Workload Breakdown</h2><h3 id="6-3-1-Methodology-and-Caveats"><a href="#6-3-1-Methodology-and-Caveats" class="headerlink" title="6.3.1 Methodology and Caveats"></a>6.3.1 Methodology and Caveats</h3><h3 id="6-3-2-Chunk-server-Workload"><a href="#6-3-2-Chunk-server-Workload" class="headerlink" title="6.3.2 Chunk-server Workload"></a>6.3.2 Chunk-server Workload</h3><h3 id="6-3-3-Appends-versus-Writes"><a href="#6-3-3-Appends-versus-Writes" class="headerlink" title="6.3.3 Appends versus Writes"></a>6.3.3 Appends versus Writes</h3><h3 id="6-3-4-Master-Workload"><a href="#6-3-4-Master-Workload" class="headerlink" title="6.3.4 Master Workload"></a>6.3.4 Master Workload</h3><h1 id="7-Experiences"><a href="#7-Experiences" class="headerlink" title="7 Experiences"></a>7 Experiences</h1><h1 id="8-Related-Work"><a href="#8-Related-Work" class="headerlink" title="8 Related Work"></a>8 Related Work</h1><h1 id="9-Conclusions"><a href="#9-Conclusions" class="headerlink" title="9 Conclusions"></a>9 Conclusions</h1>]]></content>
    
    
    <categories>
      
      <category>分布式存储论文</category>
      
      <category>Goole论文</category>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式文件系统</tag>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动态内存分配</title>
    <link href="/2021/09/24/%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"/>
    <url>/2021/09/24/%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/</url>
    
    <content type="html"><![CDATA[<h2 id="C-语言的动态内存分配"><a href="#C-语言的动态内存分配" class="headerlink" title="C 语言的动态内存分配"></a>C 语言的动态内存分配</h2><p>C语言使用<code>malloc</code>和<code>free</code>两个库函数完成动态内存的分配和释放，头文件为<code>stdlib.h</code>。其函数原型为：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-function"><span class="hljs-keyword">void</span> * <span class="hljs-title">malloc</span><span class="hljs-params">(size_t_ size)</span></span>;<br><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">free</span><span class="hljs-params">(<span class="hljs-keyword">void</span> *p)</span></span>;<br></code></pre></td></tr></table></figure><p>使用实例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;stdlib.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span><span class="hljs-meta-string">&lt;string.h&gt;</span></span><br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">char</span> * name;<br>    name = (<span class="hljs-keyword">char</span> *)<span class="hljs-built_in">malloc</span>(<span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">char</span>) * <span class="hljs-number">20</span>);<br>    <span class="hljs-built_in">strcpy</span>(name, <span class="hljs-string">&quot;Ap0l1o&quot;</span>);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%s&quot;</span>, name);<br>    <span class="hljs-built_in">free</span>(name);<br>    name = <span class="hljs-literal">NULL</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>需要注意的地方(C语言内存分配的不足之处)：</p><ul><li><code>malloc</code>函数的参数是要分配的字节数目，这个需要我们自己来计算。比如，上面的实例中，我们先使用<code>sizeof</code>函数计算了<code>char</code>类型的内存大小，然后分配了20个<code>char</code>类型内存大小；</li><li><code>malloc</code>函数的返回值是无类型指针<code>void *</code>，需要我们在程序中强制将其转为指定的类型。所以在上面的实例中，我们使用<code>(char *)</code>将其转为<code>char</code>类型的指针变量；</li><li>特别需要注意的是<code>free</code>函数只是释放了<code>malloc</code>所申请的内存，但并没有改变指针的值，因此，在释放内存后应该将指针指向<code>NULL</code>，否则该指针将指向一个无法控制的内存区域，成为野指针；</li></ul><h2 id="C-动态内存分配"><a href="#C-动态内存分配" class="headerlink" title="C++动态内存分配"></a>C++动态内存分配</h2><p>为了弥补C语言动态内存分配的缺点，C++提供了新的运算符<code>new</code>和<code>delete</code>来完成动态内存的分配和释放。</p><p>使用运算符<code>new</code>和<code>delete</code>的优点为：</p><ul><li><code>new</code>的参数为待分配单元的数目，它自动计算要分配的变量类型的大小；</li><li>它自动返回正确的指针类型，不必对返回的指针进行类型转换；</li><li>可以用<code>new</code>将分配的存储空间进行初始化；</li></ul><p>使用方法为：</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs haxe"><span class="hljs-number">1.</span>  指针变量名 = <span class="hljs-keyword">new</span> <span class="hljs-type"></span>指针基类型名(指针基类型表达式);<br>圆括号里的「指针基类型表达式」意思是用这个表达式的值来初始化指针指向的单元。<br>例如，p = <span class="hljs-keyword">new</span> <span class="hljs-type">double</span>(<span class="hljs-number">1.0</span>)，将指针p指向地址的值初始化为<span class="hljs-number">1.0</span><br><span class="hljs-number">2.</span>  指针变量名 = <span class="hljs-keyword">new</span> <span class="hljs-type"></span>指针基类型名[整型表达式];<br>方括号里的「整型表达式」是要分配一个数组，表达式的值是分配的数组的长度。<br>例如，p = <span class="hljs-keyword">new</span> <span class="hljs-type">char</span>[<span class="hljs-number">10</span>]，指针p指向长度为<span class="hljs-number">10</span>的数组的第一个元素。<br></code></pre></td></tr></table></figure><p>注意事项如下：</p><ul><li>使用<code>new</code>申请的存储空间是没有名字的，只能通过指针间接访问它们；</li><li><code>delete</code> 的操作数必须是一个<code>new</code>返回的指针；</li></ul>]]></content>
    
    
    <categories>
      
      <category>C/C++</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C/C++</tag>
      
      <tag>编程语言</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/09/21/hello-world/"/>
    <url>/2021/09/21/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    <categories>
      
      <category>读书</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
